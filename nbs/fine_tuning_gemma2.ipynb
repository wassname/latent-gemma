{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- @format -->\n",
    "\n",
    "### 1) How to Fine-tune Gemma 2 for Advanced Reasoning in Communication, Translation and Multilingual Tasks\n",
    "\n",
    "Large Language Models (LLMs) like Gemma 2 have shown remarkable capabilities, but they can struggle with complex translation and cross-lingual communication tasks that require nuanced reasoning. Traditional fine-tuning with Chain-of-Thought (CoT) provides some improvements but has inherent limitations when dealing with multilingual scenarios.\n",
    "\n",
    "In this tutorial, we'll explore an innovative approach to enhance Gemma 2's reasoning capabilities by implementing the Coconut (Chain of Continuous Thought) paradigm introduced by [Hao et al. (2024)](https://arxiv.org/pdf/2412.06769). Instead of constraining the model to reason in language space, we'll leverage continuous latent representations to enable more flexible and powerful reasoning patterns, particularly beneficial for translation and cross-lingual tasks.\n",
    "\n",
    "\n",
    "<img src=\"https://res.cloudinary.com/vickie/image/upload/v1735371553/ogylhcgz3o8trjtnjcov.png\" alt=\"Gemini Reasoning Finetuning\" width=\"1000\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some key concepts you need to know to better grasp the ideas of this tutorial\n",
    "\n",
    "##### <strong>Key Concepts</strong>\n",
    "<strong>Language Space</strong>: This is the discrete, symbolic representation of language (words, sentences).<br/>\n",
    "<strong>Continuous Thought Space (Latent Space)</strong>: This is the High-dimensional, continuous internal representations used by models for reasoning.\n",
    "\n",
    "###### <strong> Why Continuous Thought Space?</strong>\n",
    "<strong>Flexibility</strong>: Explore multiple reasoning paths simultaneously.<br/>\n",
    "<strong>Efficiency</strong>: Reduces token overhead by avoiding intermediate text. <br/>\n",
    "<strong>Nuance</strong>: Better captures complex linguistic and cross-lingual relationships.\n",
    "\n",
    "\n",
    "Having said this, let's begin by setting up the necessary environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "\n",
    "    # Core Learning Parameters\n",
    "    \"learning_rate\": 5e-5,                  # How fast the model learns (0.00005)\n",
    "    \"continuous_thoughts\": 4,               # Number of latent space reasoning steps\n",
    "    \"stages\": 4,                            # Number of training curriculum stages\n",
    "    \"training_thoughts_sequence_length\": 50, # Number of thought sequence to generate\n",
    "\n",
    "    # Inference and Evaluation Params       \n",
    "    \"fuzzy_matcher_threshold\": 80,          # Fuzzy matcher threshold at 80%\n",
    "    \"cot_decoding_k\": 5,                    # Number of paths to try before finding the best answer\n",
    "\n",
    "    # Model Setup\n",
    "    \"max_length\": 128,                      # Maximum text length to process\n",
    "    \"model_name\": \"unsloth/gemma-2-2b\",                     # Path to Gemma model\n",
    "    # \"Qwen/Qwen2.5-1.5B\",\n",
    "    # \"unsloth/Llama-3.2-1B\"\n",
    "    # \"unsloth/gemma-2-2b\", \n",
    "    \"batch_size\": 4,                        # Number of examples processed together\n",
    "    \"weight_decay\": 0.01,                   # Helps prevent overfitting\n",
    "\n",
    "    # Special Tokens\n",
    "    \"bot_id\": \"<bot>\",                      # Marks start of latent reasoning\n",
    "    \"eot_id\": \"<eot>\",                      # Marks end of latent reasoning\n",
    "    \"answer_id\": \"<answer>\",                # Marks the begining of answer\n",
    "    \"debug\": True,                          # Enables debugging output. Also allows you see the model's thoughts\n",
    "\n",
    "    # Training Optimizations\n",
    "    \"bf16\": True,                           # Uses BFloat16 for faster training\n",
    "    \"per_device_train_batch_size\": 12,       # Samples per GPU/CPU\n",
    "    \"coherence_weight\": 0.1,                 # Reasoning coherence weight\n",
    "    \"optim\": \"adamw_torch\",                 # AdamW optimizer for efficiency\n",
    "    \"wandb_project\": \"gemma2-finetuning\",   # Tracks training on Weights & Biases\n",
    "    \"logging_steps\": 1,                     # How often to log training progress\n",
    "    \"bf16_full_eval\": True,                 # Uses BFloat16 for evaluation\n",
    "    \"gradient_accumulation_steps\": 1,       # How often to update weights\n",
    "    \"save_steps\": 10000,                    # How often to save model\n",
    "    \"warmup_steps\": 0.1,                    # Number of warmup steps\n",
    "    \"output_dir\": \"../output\",                 # Where to save model files\n",
    "    \"diversity_weight\": 0.1,                # Reasoning diversity weight\n",
    "    \"num_train_epochs\": 3,                  # Number of training epochs\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Dataset Overview: Japanese-English Translation/Communication\n",
    "<i>Using the llm-japanese-dataset created by [Hirano et al. (2023)](https://arxiv.org/pdf/2305.12720)</i>\n",
    "\n",
    "We'll be using the llm-japanese-dataset (8.4M records) to fine-tune Gemma 2, focusing on Japanese-English translation and communication. The dataset follows this format:\n",
    "\n",
    "```\n",
    "### Instruction:\n",
    "Please translate to English.\n",
    "\n",
    "### Input:\n",
    "こんにちは、元気ですか？\n",
    "\n",
    "### Response:\n",
    "Hello, how are you?\n",
    "```\n",
    "\n",
    "Most of the data (about 80%) consists of translations like this, making it perfect for improving Gemma 2's Japanese-English capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T18:12:09.715221Z",
     "iopub.status.busy": "2024-12-16T18:12:09.714341Z",
     "iopub.status.idle": "2024-12-16T18:13:04.760441Z",
     "shell.execute_reply": "2024-12-16T18:13:04.759432Z",
     "shell.execute_reply.started": "2024-12-16T18:12:09.715185Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from datasets import config as dataset_config\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "dataset_name = \"izumi-lab/llm-japanese-dataset\"\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# For this tutorial, let's take N samples from the dataset\n",
    "DS_SIZE = 3000\n",
    "\n",
    "truncated_dataset = DatasetDict({\n",
    "    split: dataset[split].select(range(DS_SIZE))\n",
    "    for split in dataset.keys()\n",
    "})\n",
    "\n",
    "\n",
    "dataset = truncated_dataset\n",
    "eval_dataset = dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see few examples of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T18:13:04.790751Z",
     "iopub.status.busy": "2024-12-16T18:13:04.790424Z",
     "iopub.status.idle": "2024-12-16T18:13:04.805684Z",
     "shell.execute_reply": "2024-12-16T18:13:04.804873Z",
     "shell.execute_reply.started": "2024-12-16T18:13:04.790716Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:  「abc ～the first～」へようこそ！さて、ABC・・・と始まるアルファベットは、全部で何文字でしょう？ \n",
      "\n",
      "Input:   \n",
      "\n",
      "Output:  26文字 \n",
      "\n",
      "========================================================================================================================================================================================================\n",
      "\n",
      "Instruction:  人気漫画『ドラえもん』の登場人物で、ジャイアンの苗字は剛田ですが、スネ夫の苗字は何でしょう？ \n",
      "\n",
      "Input:   \n",
      "\n",
      "Output:  骨川（滑川も正解） \n",
      "\n",
      "========================================================================================================================================================================================================\n",
      "\n",
      "Instruction:  格闘家ボブ・サップの出身国はどこでしょう？ \n",
      "\n",
      "Input:   \n",
      "\n",
      "Output:  アメリカ \n",
      "\n",
      "========================================================================================================================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(\"Instruction: \", dataset['train'][\"instruction\"][i], \"\\n\")\n",
    "    print(\"Input: \", dataset['train'][\"input\"][i], \"\\n\")\n",
    "    print(\"Output: \",dataset['train'][\"output\"][i], \"\\n\")\n",
    "    print(f\"{'='*200}\\n\")\n",
    "\n",
    "1/0 # FIXME why no input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5) Language Detector\n",
    "\n",
    "When handling multilingual content, detecting languages accurately is crucial. Our language detector analyzes both the structure and composition of mixed-language text.\n",
    "\n",
    "For example, let's examine this input:\n",
    "```python\n",
    "\"「abc ～the first～」へようこそ！さて、ABC・・・と始まるアルファベットは、全部で何文字でしょう？\"\n",
    "```\n",
    "\n",
    "The detector identifies Japanese as the primary language while recognizing English phrases (abc, first, ABC) embedded within. It analyzes the distribution of scripts including Hiragana, Latin alphabet, and various symbols. This composition analysis helps determine language percentages and structure.\n",
    "\n",
    "Beyond basic detection, this analysis guides translation strategy and validates output language alignment. By understanding the input language composition, Gemma 2 can better reason about how to process and generate appropriate bilingual responses. The detector becomes especially valuable when building chain-of-thought reasoning patterns across languages.\n",
    "\n",
    "Let's implement this detector..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T18:15:41.502874Z",
     "iopub.status.busy": "2024-12-16T18:15:41.502091Z",
     "iopub.status.idle": "2024-12-16T18:15:41.517010Z",
     "shell.execute_reply": "2024-12-16T18:15:41.515979Z",
     "shell.execute_reply.started": "2024-12-16T18:15:41.502815Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: スナフキン ===>>>> {'Japanese': 100.0}\n",
      "Text: レベッカ(REBECCA) ===>>>> {'English': 63.6, 'Japanese': 36.4}\n",
      "Text: Hello World ===>>>> {'English': 100.0}\n",
      "Text: こんにちは World! ===>>>> {'Japanese': 50.0, 'English': 50.0}\n"
     ]
    }
   ],
   "source": [
    "# Implementing the LanguageDetector class\n",
    "\n",
    "from typing import Dict, List, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ScriptRange:\n",
    "    \"\"\"Represents a Unicode range for a writing system\"\"\"\n",
    "    start: int\n",
    "    end: int\n",
    "    name: str\n",
    "    \n",
    "class LanguageDetector:\n",
    "    def __init__(self):\n",
    "        self.scripts: List[ScriptRange] = []\n",
    "        self.language_mappings: Dict[str, List[str]] = {}\n",
    "        \n",
    "    def add_script(self, name: str, start: int, end: int) -> None:\n",
    "        \"\"\"\n",
    "        Add a new script range to the detector\n",
    "        \n",
    "        Args:\n",
    "            name: Name of the script (e.g., 'Hiragana', 'Latin')\n",
    "            start: Starting Unicode code point\n",
    "            end: Ending Unicode code point\n",
    "        \"\"\"\n",
    "        self.scripts.append(ScriptRange(start, end, name))\n",
    "    \n",
    "    def map_scripts_to_language(self, language: str, script_names: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Map multiple scripts to a single language\n",
    "        \n",
    "        Args:\n",
    "            language: Name of the language (e.g., 'Japanese')\n",
    "            script_names: List of script names that belong to this language\n",
    "        \"\"\"\n",
    "        self.language_mappings[language] = script_names\n",
    "    \n",
    "    def detect(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Detect the percentage of different languages/scripts in the text\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to analyze\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping language/script names to their percentage presence\n",
    "        \"\"\"\n",
    "        # Count characters in each script\n",
    "        char_counts: Dict[str, int] = {script.name: 0 for script in self.scripts}\n",
    "        total_chars = 0\n",
    "        \n",
    "        for char in text:\n",
    "            if char.isspace() or char in '.,!?()[]{}':\n",
    "                continue\n",
    "                \n",
    "            code = ord(char)\n",
    "            total_chars += 1\n",
    "            \n",
    "            # Check which script range the character falls into\n",
    "            for script in self.scripts:\n",
    "                if script.start <= code <= script.end:\n",
    "                    char_counts[script.name] += 1\n",
    "                    break\n",
    "        \n",
    "        if total_chars == 0:\n",
    "            return {}\n",
    "            \n",
    "        # Calculate initial percentages\n",
    "        percentages = {\n",
    "            script: (count / total_chars) * 100\n",
    "            for script, count in char_counts.items()\n",
    "            if count > 0\n",
    "        }\n",
    "        \n",
    "        # Combine scripts into languages where applicable\n",
    "        final_percentages = {}\n",
    "        used_scripts = set()\n",
    "        \n",
    "        # First, handle mapped languages\n",
    "        for language, script_names in self.language_mappings.items():\n",
    "            total = sum(percentages.get(script, 0) for script in script_names)\n",
    "            if total > 0:\n",
    "                final_percentages[language] = total\n",
    "                used_scripts.update(script_names)\n",
    "        \n",
    "        # Then add remaining unmapped scripts\n",
    "        for script, percentage in percentages.items():\n",
    "            if script not in used_scripts:\n",
    "                final_percentages[script] = percentage\n",
    "        \n",
    "        return {k: round(v, 1) for k, v in sorted(\n",
    "            final_percentages.items(),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )}\n",
    "\n",
    "# Example setup and usage\n",
    "def create_default_detector() -> LanguageDetector:\n",
    "    \"\"\"Create a detector with Japanese and English support\"\"\"\n",
    "    detector = LanguageDetector()\n",
    "    \n",
    "    # Add Japanese scripts\n",
    "    detector.add_script('Hiragana', 0x3040, 0x309F)\n",
    "    detector.add_script('Katakana', 0x30A0, 0x30FF)\n",
    "    detector.add_script('Kanji', 0x4E00, 0x9FFF)\n",
    "\n",
    "    # Add English scripts\n",
    "    detector.add_script('Latin', 0x0000, 0x024F)\n",
    "\n",
    "    \n",
    "    # Map scripts to languages\n",
    "    detector.map_scripts_to_language('Japanese', ['Hiragana', 'Katakana', 'Kanji'])\n",
    "    detector.map_scripts_to_language('English', ['Latin'])\n",
    "    \n",
    "    return detector\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    detector = create_default_detector()\n",
    "    \n",
    "    test_texts = [\n",
    "        'スナフキン',\n",
    "        'レベッカ(REBECCA)',\n",
    "        'Hello World',\n",
    "        'こんにちは World!'\n",
    "    ]\n",
    "    \n",
    "    for text in test_texts:\n",
    "        result = detector.detect(text)\n",
    "        print(f\"Text: {text} ===>>>> {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6) Dataset Preprocessing\n",
    "\n",
    "The preprocessing pipeline prepares our dataset for Continuous Latent Reasoning by transforming raw translation pairs into structured training data. Here's what each component does:\n",
    "\n",
    "`preprocess_function`: \n",
    "Takes raw examples and generates Chain-of-Thought reasoning steps. For each sample, it:\n",
    "1. Analyzes language composition of input/output\n",
    "2. Generates appropriate reasoning steps in detected language\n",
    "3. Formats with special tokens (bot_token, eot_token) as per Hao et al. (2024)\n",
    "\n",
    "Example flow:\n",
    "```python\n",
    "Input: \"「abc ～the first～」へようこそ！\"\n",
    "Steps:\n",
    "- Detect languages (Japanese: 60%, English: 40%)\n",
    "- Generate understanding steps\n",
    "- Format with special tokens\n",
    "Output: \"<bos> Input <eos><bot><eot> Step 1... Step 2... Answer <eos>\"\n",
    "```\n",
    "\n",
    "`tokenizer_function`:\n",
    "Converts text into model inputs by:\n",
    "1. Tokenizing the formatted text\n",
    "2. Creating attention masks\n",
    "3. Preparing labels (masking question/thought tokens)\n",
    "\n",
    "\n",
    "This preprocessing ensures our data is properly structured for training Gemma 2 in continuous latent space reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "def preprocess_function(\n",
    "    examples, \n",
    "    detector=None,  # Make detector optional\n",
    "    stages=1, \n",
    "    eos_token=\"<eos>\",\n",
    "    bos_token=\"<bos>\",\n",
    "    language_config=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Preprocess the input examples by constructing the prompt with reasoning steps.\n",
    "\n",
    "    Args:\n",
    "        examples (dict): A dictionary containing the input examples with keys \"instruction\", \"input\", and \"output\".\n",
    "        detector: A language detection object or function that detects the language of a given text.\n",
    "        stages (int): The number of reasoning stages to include in the prompt.\n",
    "        eos_token (str): The end-of-sequence token.\n",
    "        bos_token (str): The beginning-of-sequence token.\n",
    "        language_config (dict): A dictionary mapping language keys to their respective translations for steps and labels.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the preprocessed prompts.\n",
    "    \"\"\"\n",
    "\n",
    "    if language_config is None:\n",
    "        language_config = {\n",
    "            \"English\": {\n",
    "                \"language_detection\": \"Question language detection\",\n",
    "                \"understand_question\": \"Understand the question\",\n",
    "                \"understand_answer\": \"Understand the answer\",\n",
    "                \"response_language_detection\": \"Response language detection\",\n",
    "                \"answer_label\": \"Answer:\",\n",
    "                \"step_label\": \"Step\",\n",
    "            },\n",
    "            \"Japanese\": {\n",
    "                \"language_detection\": \"言語の検出\",\n",
    "                \"understand_question\": \"質問を理解する\",\n",
    "                \"understand_answer\": \"答えを理解する\",\n",
    "                \"response_language_detection\": \"応答言語の検出\",\n",
    "                \"answer_label\": \"答え：\",\n",
    "                \"step_label\": \"ステップ\",\n",
    "            },\n",
    "            # Add more languages here as needed\n",
    "        }\n",
    "\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "\n",
    "    bot = config[\"bot_id\"]\n",
    "    eot = config[\"eot_id\"]\n",
    "    answer_token = config[\"answer_id\"]\n",
    "\n",
    "    # Initialize output dictionaries with lists\n",
    "    result = []\n",
    "\n",
    "    for i in range(len(instructions)):\n",
    "        instruction = instructions[i]\n",
    "        input = inputs[i]\n",
    "        output = outputs[i]\n",
    "\n",
    "        if len(input) > 1:\n",
    "            input = instruction + input\n",
    "        else:\n",
    "            input = instruction\n",
    "\n",
    "        # Use the provided detector to detect languages\n",
    "        input_language = detector.detect(input) if detector else {\"English\": 100.0}  # Default to English if no detector\n",
    "        output_language = detector.detect(output) if detector else {\"English\": 100.0}  # Default to English if no detector\n",
    "\n",
    "        steps = []\n",
    "\n",
    "        # Determine the primary input and output languages\n",
    "        # Use the language key from the detector's output that matches a key in language_config\n",
    "        input_lang = next((lang for lang in input_language if lang in language_config), \"English\")\n",
    "        output_lang = next((lang for lang in output_language if lang in language_config), \"English\")\n",
    "\n",
    "        # Get the language-specific labels\n",
    "        input_labels = language_config.get(input_lang, language_config[\"English\"])\n",
    "        output_labels = language_config.get(output_lang, language_config[\"English\"])\n",
    "\n",
    "        # Input language detection\n",
    "        input_lang_str = \", \".join([f\"{k}: {v}%\" for k, v in input_language.items()])\n",
    "        steps.append(f\"{input_labels['language_detection']}: {input_lang_str}\")\n",
    "        steps.append(f\"{input_labels['understand_question']}: {input}\")\n",
    "        steps.append(f\"{input_labels['understand_answer']}: {output}\")\n",
    "\n",
    "        # Output language detection\n",
    "        output_lang_str = \", \".join([f\"{k}: {v}%\" for k, v in output_language.items()]) if output_language else \"Unknown\"\n",
    "        steps.append(f\"{output_labels['response_language_detection']}: {output_lang_str}\")\n",
    "\n",
    "        # Format steps with step numbers\n",
    "        steps = [f\"{output_labels['step_label']} {i+1} : {step}\" for i, step in enumerate(steps)]\n",
    "\n",
    "        # Include only the steps relevant to the current stage\n",
    "        if stages > 0:\n",
    "            steps = steps[-stages:]  # Keep the last `stages` steps\n",
    "\n",
    "        # Renumber steps to start from 1\n",
    "        steps = [f\"{output_labels['step_label']} {i+1} : {step.split(' : ')[1]}\" for i, step in enumerate(steps)]\n",
    "\n",
    "        # Construct the prompt\n",
    "        prompt = bos_token + \"\\n\" + input + eos_token + bot + eot + \"\\n\" + \"\\n\".join(steps) + \"\\n\" + answer_token + output_labels['answer_label'] + output + eos_token\n",
    "\n",
    "        result.append(prompt)\n",
    "\n",
    "    return {\n",
    "        \"prompt\": result\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def tokenizer_function(examples, tokenizer):\n",
    "    \"\"\"\n",
    "    Tokenize the input prompt and prepare the input_ids, attention_mask, and labels for training.\n",
    "\n",
    "    Args:\n",
    "        examples (dict): A dictionary containing the input prompts.\n",
    "        tokenizer (PreTrainedTokenizer): The tokenizer to use for tokenization.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the tokenized input_ids, attention_mask, and labels.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = examples[\"prompt\"]\n",
    "    eot = config[\"eot_id\"]\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        prompt,\n",
    "        max_length=config[\"max_length\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    input_ids = tokenized[\"input_ids\"].squeeze(0)\n",
    "    attention_mask = tokenized[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "    labels = input_ids.clone()\n",
    "    batch_size = labels.shape[0]\n",
    "    eot_id = tokenizer.convert_tokens_to_ids(eot)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Find the positions of <eot> in the input_ids\n",
    "        eot_pos = (input_ids[i] == eot_id).nonzero(as_tuple=True)\n",
    "\n",
    "        if len(eot_pos[0]) > 0:\n",
    "            # Get the last occurrence of <eot>\n",
    "            last_eot_pos = eot_pos[0][-1].item()\n",
    "            \n",
    "            # Mask everything before and including the last <eot>\n",
    "            labels[i, :last_eot_pos] = -100\n",
    "\n",
    "        # Mask padding\n",
    "        labels[i, attention_mask[i] == 0] = -100\n",
    "\n",
    "\n",
    "    value =  {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "    }\n",
    "\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        return value\n",
    "    else:\n",
    "        value[\"labels\"] = labels\n",
    "        return value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language Config. If you need to handle more languages, you can add to this configuration and pick a suitable language detector as `LanguageDetector` class only support `English` and `Japanese` ... Note the keys `English`, `Japanese` or any other you intend to include must match the response from your Language detector `Text: レベッカ(REBECCA) ===>>>> {'English': 63.6, 'Japanese': 36.4}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_config = {\n",
    "    \"English\": {\n",
    "        \"language_detection\": \"Question language detection\",\n",
    "        \"understand_question\": \"Understand the question\",\n",
    "        \"understand_answer\": \"Understand the answer\",\n",
    "        \"response_language_detection\": \"Response language detection\",\n",
    "        \"answer_label\": \"Answer:\",\n",
    "        \"step_label\": \"Step\",\n",
    "    },\n",
    "    \"Japanese\": {\n",
    "        \"language_detection\": \"言語の検出\",\n",
    "        \"understand_question\": \"質問を理解する\",\n",
    "        \"understand_answer\": \"答えを理解する\",\n",
    "        \"response_language_detection\": \"応答言語の検出\",\n",
    "        \"answer_label\": \"答え：\",\n",
    "        \"step_label\": \"ステップ\",\n",
    "    },\n",
    "    # Add more languages here as needed\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize what our dataset looks like preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5afc4e97e8a4320937172f06106185b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage: ========================>>>>>>>>>>>>>>>>> 0\n",
      "Input:  <bos>\n",
      "「abc ～the first～」へようこそ！さて、ABC・・・と始まるアルファベットは、全部で何文字でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 59.3%, English: 25.9%\n",
      "ステップ 2 : 質問を理解する: 「abc ～the first～」へようこそ！さて、ABC・・・と始まるアルファベットは、全部で何文字でしょう？\n",
      "ステップ 3 : 答えを理解する: 26文字\n",
      "ステップ 4 : 応答言語の検出: Japanese: 50.0%, English: 50.0%\n",
      "<answer>答え：26文字<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "人気漫画『ドラえもん』の登場人物で、ジャイアンの苗字は剛田ですが、スネ夫の苗字は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 89.1%\n",
      "ステップ 2 : 質問を理解する: 人気漫画『ドラえもん』の登場人物で、ジャイアンの苗字は剛田ですが、スネ夫の苗字は何でしょう？\n",
      "ステップ 3 : 答えを理解する: 骨川（滑川も正解）\n",
      "ステップ 4 : 応答言語の検出: Japanese: 77.8%\n",
      "<answer>答え：骨川（滑川も正解）<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "格闘家ボブ・サップの出身国はどこでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 95.2%\n",
      "ステップ 2 : 質問を理解する: 格闘家ボブ・サップの出身国はどこでしょう？\n",
      "ステップ 3 : 答えを理解する: アメリカ\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：アメリカ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ロシア語で「城」という意味がある、ロシアの大統領府の別名は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 88.6%\n",
      "ステップ 2 : 質問を理解する: ロシア語で「城」という意味がある、ロシアの大統領府の別名は何でしょう？\n",
      "ステップ 3 : 答えを理解する: クレムリン\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：クレムリン<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "織田信長、豊臣秀吉、徳川家康という３人の戦国武将の性格を表現するのに用いられる鳥は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 91.5%\n",
      "ステップ 2 : 質問を理解する: 織田信長、豊臣秀吉、徳川家康という３人の戦国武将の性格を表現するのに用いられる鳥は何でしょう？\n",
      "ステップ 3 : 答えを理解する: ホトトギス\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ホトトギス<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "524c00998cf64ed59bc01b21ba84517f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage: ========================>>>>>>>>>>>>>>>>> 1\n",
      "Input:  <bos>\n",
      "「abc ～the first～」へようこそ！さて、ABC・・・と始まるアルファベットは、全部で何文字でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 50.0%, English: 50.0%\n",
      "<answer>答え：26文字<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "人気漫画『ドラえもん』の登場人物で、ジャイアンの苗字は剛田ですが、スネ夫の苗字は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 77.8%\n",
      "<answer>答え：骨川（滑川も正解）<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "格闘家ボブ・サップの出身国はどこでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：アメリカ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ロシア語で「城」という意味がある、ロシアの大統領府の別名は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：クレムリン<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "織田信長、豊臣秀吉、徳川家康という３人の戦国武将の性格を表現するのに用いられる鳥は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ホトトギス<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dadf48b1888b4647b7c9c143bfbcbc2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage: ========================>>>>>>>>>>>>>>>>> 2\n",
      "Input:  <bos>\n",
      "「abc ～the first～」へようこそ！さて、ABC・・・と始まるアルファベットは、全部で何文字でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 26文字\n",
      "ステップ 2 : 応答言語の検出: Japanese: 50.0%, English: 50.0%\n",
      "<answer>答え：26文字<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "人気漫画『ドラえもん』の登場人物で、ジャイアンの苗字は剛田ですが、スネ夫の苗字は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 骨川（滑川も正解）\n",
      "ステップ 2 : 応答言語の検出: Japanese: 77.8%\n",
      "<answer>答え：骨川（滑川も正解）<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "格闘家ボブ・サップの出身国はどこでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: アメリカ\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：アメリカ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ロシア語で「城」という意味がある、ロシアの大統領府の別名は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: クレムリン\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：クレムリン<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "織田信長、豊臣秀吉、徳川家康という３人の戦国武将の性格を表現するのに用いられる鳥は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: ホトトギス\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ホトトギス<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ec0ea0610c4db197521df74536d486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage: ========================>>>>>>>>>>>>>>>>> 3\n",
      "Input:  <bos>\n",
      "「abc ～the first～」へようこそ！さて、ABC・・・と始まるアルファベットは、全部で何文字でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 「abc ～the first～」へようこそ！さて、ABC・・・と始まるアルファベットは、全部で何文字でしょう？\n",
      "ステップ 2 : 答えを理解する: 26文字\n",
      "ステップ 3 : 応答言語の検出: Japanese: 50.0%, English: 50.0%\n",
      "<answer>答え：26文字<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "人気漫画『ドラえもん』の登場人物で、ジャイアンの苗字は剛田ですが、スネ夫の苗字は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 人気漫画『ドラえもん』の登場人物で、ジャイアンの苗字は剛田ですが、スネ夫の苗字は何でしょう？\n",
      "ステップ 2 : 答えを理解する: 骨川（滑川も正解）\n",
      "ステップ 3 : 応答言語の検出: Japanese: 77.8%\n",
      "<answer>答え：骨川（滑川も正解）<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "格闘家ボブ・サップの出身国はどこでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 格闘家ボブ・サップの出身国はどこでしょう？\n",
      "ステップ 2 : 答えを理解する: アメリカ\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：アメリカ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ロシア語で「城」という意味がある、ロシアの大統領府の別名は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: ロシア語で「城」という意味がある、ロシアの大統領府の別名は何でしょう？\n",
      "ステップ 2 : 答えを理解する: クレムリン\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：クレムリン<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "織田信長、豊臣秀吉、徳川家康という３人の戦国武将の性格を表現するのに用いられる鳥は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 織田信長、豊臣秀吉、徳川家康という３人の戦国武将の性格を表現するのに用いられる鳥は何でしょう？\n",
      "ステップ 2 : 答えを理解する: ホトトギス\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ホトトギス<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# So we do not load every dataset as this takes a while\n",
    "truncated_dataset = DatasetDict({\n",
    "    split: dataset[split].select(range(5))\n",
    "    for split in dataset.keys()\n",
    "})\n",
    "\n",
    "for stage in range(config[\"stages\"]):\n",
    "    dataset_ = truncated_dataset.map(\n",
    "        (lambda x: preprocess_function(\n",
    "            x, \n",
    "            detector=detector,\n",
    "            stages=stage, \n",
    "            language_config=language_config\n",
    "        )),\n",
    "        batched=True,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "    )\n",
    "\n",
    "    print(f\"Stage: ========================>>>>>>>>>>>>>>>>> {stage}\")\n",
    "    for i in range(5):\n",
    "        print(\"Input: \", dataset_['train'][\"prompt\"][i], \"\\n\")\n",
    "        print(f\"{'='*100}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6) Modelling\n",
    "\n",
    "Our `LatentReasoningGemmaForCausalLM` extends GemmaForCausalLM to enable continuous latent reasoning, following the architecture from Hao et al. (2024). The model implements two key forward paths:\n",
    "\n",
    "`infer_forward`: During inference, transforms input text into continuous thought representations before generating the final output. It maintains a chain of latent states between the `<bot>` and `<eot>` tokens, allowing for more nuanced reasoning across languages.\n",
    "\n",
    "`train_forward`: During training, processes the sequence in stages, gradually building up continuous thought representations while masking appropriate parts of the input. It helps the model learn to reason in latent space while maintaining language understanding.\n",
    "\n",
    "Let's proceed with the creation of our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import GemmaForCausalLM, DynamicCache, PreTrainedTokenizer\n",
    "from typing import Optional, List, Union, Dict, Any\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LatentReasoningGemmaForCausalLM(GemmaForCausalLM):\n",
    "    \"\"\"\n",
    "    A custom implementation of GemmaForCausalLM that supports latent reasoning \n",
    "    using the Coconut (Chain of Continuous Thought) paradigm.\n",
    "    \"\"\"\n",
    "\n",
    "    DEFAULT_CONFIG = {\n",
    "        # Core Learning Parameters\n",
    "        \"continuous_thoughts\": 4,               # Number of latent space reasoning steps\n",
    "        \"stages\": 4,                            # Number of training curriculum stages\n",
    "        \"training_thoughts_sequence_length\": 50, # Number of thought sequence to generate\n",
    "\n",
    "        # Inference and Evaluation Params       \n",
    "        \"fuzzy_matcher_threshold\": 80,          # Fuzzy matcher threshold at 80%\n",
    "        \"cot_decoding_k\": 5,                    # Number of paths to try before finding the best answer\n",
    "\n",
    "        # Model Setup\n",
    "        \"max_length\": config['max_length'],                      # Maximum text length to process\n",
    "\n",
    "        # Special Tokens\n",
    "        \"bot_id\": \"<bot>\",                      # Marks start of latent reasoning\n",
    "        \"eot_id\": \"<eot>\",                      # Marks end of latent reasoning\n",
    "        \"answer_id\": \"<answer>\",                # Marks the begining of answer\n",
    "        \"debug\": True,                          # Enables debugging output. Also allows you see the model's thoughts\n",
    "\n",
    "    }\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.tokenizer: PreTrainedTokenizer = None\n",
    "        self.current_stage = 0\n",
    "        self.model_config = type(self).DEFAULT_CONFIG\n",
    "        self.debug = self.model_config.get(\"debug\", False)\n",
    "        self.diversity_weight = self.model_config.get(\"diversity_weight\", 0.1)\n",
    "        self.coherence_weight = self.model_config.get(\"coherence_weight\", 0.1)\n",
    "\n",
    "    def get_input_ids(self, inputs_embeds):\n",
    "        \"\"\"Helper method to get input ids from embeddings.\"\"\"\n",
    "        embedding_matrix = self.get_input_embeddings().weight\n",
    "        similarities = torch.matmul(inputs_embeds, embedding_matrix.T)\n",
    "        token_ids = torch.argmax(similarities, dim=-1)\n",
    "        return token_ids\n",
    "\n",
    "    def thoughts_forward(self, num_thoughts, thought_ids, thought_mask, num_of_thought_tokens = 1):\n",
    "        \"\"\"\n",
    "        Generate continuous thought embeddings.\n",
    "        \"\"\"\n",
    "        all_thought_outputs = []\n",
    "        batch_size = thought_ids.shape[0]\n",
    "        \n",
    "        # Get initial embeddings\n",
    "        initial_embeds = self.get_input_embeddings()(thought_ids)\n",
    "        current_embeds = initial_embeds\n",
    "        current_mask = thought_mask\n",
    "\n",
    "        for t in range(num_thoughts):\n",
    "            # Forward pass through transformer\n",
    "            outputs = self.model.forward(\n",
    "                inputs_embeds=current_embeds,\n",
    "                attention_mask=current_mask,\n",
    "                past_key_values=None,\n",
    "                use_cache=False,\n",
    "                return_dict=True,\n",
    "                output_hidden_states=True,  # Get hidden states from all layers\n",
    "            )\n",
    "            \n",
    "            # Get hidden states from all layers for better representation\n",
    "            hidden_states = outputs.hidden_states\n",
    "\n",
    "\n",
    "            # TODO: try non random attn and projections below\n",
    "            # TODO: use einops\n",
    "            \n",
    "            # TODO: try using only supressed activations. These are the values that disapear in the last layer. So instead of a random weight we would \n",
    "            # see transforms in https://github.com/wassname/repr-preference-optimization/blob/runpod/reprpo/interventions/transforms/supressed.py\n",
    "            \"\"\"\n",
    "\n",
    "            Here we define a transform to isolate supressed activations, where we hypothesis that style/concepts/scratchpads and other internal only representations must be stored.\n",
    "\n",
    "            See the following references for more information:\n",
    "            - https://arxiv.org/html/2406.19384v1\n",
    "                - > Previous work suggests that networks contain ensembles of “prediction\" neurons, which act as probability promoters [66, 24, 32] and work in tandem with suppression neurons (Section 5.4). \n",
    "\n",
    "            - https://arxiv.org/pdf/2401.12181\n",
    "                > We find a striking pattern which is remarkably consistent across the different seeds: after about the halfway point in the model, prediction neurons become increasingly prevalent until the very end of the network where there is a sudden shift towards a much larger number of suppression neurons.\n",
    "            \"\"\"\n",
    "            weighted_states = (hidden_states[-2] - hidden_states[-1]).clamp(0, None) # [batch_size, seq_len, hidden_size] \n",
    "\n",
    "            # # OR: Combine hidden states from different layers using random attention\n",
    "            # layer_attention = torch.softmax(\n",
    "            #     torch.randn(len(hidden_states), device=hidden_states[0].device), \n",
    "            #     dim=0\n",
    "            # )\n",
    "            # weighted_states = sum(w * h for w, h in zip(layer_attention, hidden_states))\n",
    "            \n",
    "            n = num_of_thought_tokens\n",
    "            last_hidden = weighted_states[:, -n:, :]  # [batch_size, n, hidden_size]\n",
    "            \n",
    "            # Project to lower dimension for thought space using random vector\n",
    "            thought_proj = nn.Sequential(\n",
    "                nn.Linear(last_hidden.shape[-1], self.config.hidden_size // 2),\n",
    "                nn.LayerNorm(self.config.hidden_size // 2),\n",
    "                nn.GELU()\n",
    "            ).to(last_hidden.device)\n",
    "            projected_thought = thought_proj(last_hidden)  # [batch_size, n, hidden_size // 2]\n",
    "            \n",
    "            # Add noise to increase diversity\n",
    "            noise = torch.randn_like(projected_thought) * 0.1  # Adjust noise scale as needed\n",
    "            projected_thought = projected_thought + noise\n",
    "            \n",
    "            # Project back to embedding space\n",
    "            embed_proj = nn.Linear(\n",
    "                self.config.hidden_size // 2,\n",
    "                self.config.hidden_size,\n",
    "                device=projected_thought.device\n",
    "            )\n",
    "            next_token_embeds = embed_proj(projected_thought)  # [batch_size, n, hidden_size]\n",
    "            \n",
    "            # Apply layer normalization for stability\n",
    "            next_token_embeds = nn.LayerNorm(\n",
    "                self.config.hidden_size,\n",
    "                device=next_token_embeds.device\n",
    "            )(next_token_embeds)\n",
    "\n",
    "            # TODO consider positional encoding so differentiate between thoughts and input as they might have very differen't?\n",
    "            \n",
    "            # Update embeddings and mask\n",
    "            current_embeds = torch.cat([current_embeds, next_token_embeds], dim=1)\n",
    "            current_mask = torch.cat([\n",
    "                current_mask,\n",
    "                torch.ones((batch_size, n), device=current_mask.device)\n",
    "            ], dim=1)\n",
    "            \n",
    "            all_thought_outputs.append(last_hidden)\n",
    "\n",
    "        # Ensure reasonable sequence length\n",
    "        max_seq_len = self.model_config.get(\"max_length\", 512)\n",
    "        if current_embeds.shape[1] > max_seq_len:\n",
    "            current_embeds = current_embeds[:, :max_seq_len, :]\n",
    "            current_mask = current_mask[:, :max_seq_len]\n",
    "        \n",
    "        return all_thought_outputs, current_embeds, current_mask\n",
    "\n",
    "\n",
    "    def train_forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        num_logits_to_keep: int = 0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Training forward pass with continuous thought generation and CoT alignment.\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "\n",
    "        # Keep original labels if none provided\n",
    "        if labels is None:\n",
    "            labels = input_ids.clone()\n",
    "            batch_size = labels.shape[0]\n",
    "            eot_id = self.tokenizer.convert_tokens_to_ids(self.model_config[\"eot_id\"])\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                # Find the positions of <eot> in the input_ids\n",
    "                eot_pos = (input_ids[i] == eot_id).nonzero(as_tuple=True)\n",
    "\n",
    "                if len(eot_pos[0]) > 0:\n",
    "                    # Get the last occurrence of <eot>\n",
    "                    last_eot_pos = eot_pos[0][-1].item()\n",
    "                    \n",
    "                    # Mask everything before and including the last <eot>\n",
    "                    labels[i, :last_eot_pos] = -100\n",
    "\n",
    "                # Mask padding\n",
    "                labels[i, attention_mask[i] == 0] = -100\n",
    "\n",
    "        # Get input embeddings if not provided\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.get_input_embeddings()(input_ids)\n",
    "\n",
    "\n",
    "        # Generate continuous thoughts\n",
    "        if self.current_stage > 0:\n",
    "            num_thoughts = self.current_stage * self.model_config[\"continuous_thoughts\"]\n",
    "            all_thoughts, final_embeds, final_mask = self.thoughts_forward(\n",
    "                num_thoughts=num_thoughts,\n",
    "                thought_ids=input_ids,\n",
    "                thought_mask=attention_mask,\n",
    "                num_of_thought_tokens = self.model_config[\"training_thoughts_sequence_length\"]\n",
    "            )\n",
    "\n",
    "            # Add auxiliary losses\n",
    "            auxiliary_losses = []\n",
    "\n",
    "            # Thought coherence loss\n",
    "            if len(all_thoughts) > 1:\n",
    "                coherence_loss = 0\n",
    "                for t1, t2 in zip(all_thoughts[:-1], all_thoughts[1:]):\n",
    "                    sim = F.cosine_similarity(t1, t2, dim=-1)\n",
    "                    coherence_loss += (1 - sim).mean()\n",
    "                auxiliary_losses.append(coherence_loss * self.coherence_weight)\n",
    "\n",
    "            batch_size = labels.shape[0]\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                # Find the start and end of CoT in the labels\n",
    "                cot_start = None\n",
    "                \n",
    "                for j, token_id in enumerate(labels[i]):\n",
    "                    if token_id == self.tokenizer.convert_tokens_to_ids(self.model_config[\"eot_id\"]):\n",
    "                        cot_start = j + 1  # Start of CoT\n",
    "\n",
    "\n",
    "                # Debugging: Print CoT tokens and latent thoughts\n",
    "                if cot_start is not None:\n",
    "                    # Extract CoT tokens\n",
    "                    cot_tokens = labels[i, cot_start:]  # [cot_seq_len]\n",
    "\n",
    "                    # Get the latent thoughts for this batch\n",
    "                    latent_thoughts = all_thoughts[i]  # [thought_seq_len, hidden_size]\n",
    "\n",
    "                    # Project latent thoughts to logits\n",
    "                    thought_logits = self.lm_head(latent_thoughts)  # [thought_seq_len, vocab_size]\n",
    "                    thought_token_ids = torch.argmax(thought_logits, dim=-1)  # [thought_seq_len]\n",
    "\n",
    "\n",
    "                    # Debugging: Print CoT tokens and latent thoughts\n",
    "                    if self.debug:\n",
    "                        # Decode CoT tokens\n",
    "                        cot_tokens_list = cot_tokens.squeeze().tolist()  # Convert to 1D list\n",
    "                        if isinstance(cot_tokens_list, int):  # Handle single token case\n",
    "                            cot_tokens_list = [cot_tokens_list]\n",
    "                        cot_text = self.tokenizer.decode(cot_tokens_list, skip_special_tokens=True)\n",
    "                        print(f\" ==================== \\n Debug: CoT for batch {i}: {cot_text} \\n ====================\")\n",
    "\n",
    "                        # Decode latent thoughts\n",
    "                        thought_token_ids_list = thought_token_ids.squeeze().tolist()  # Convert to list\n",
    "\n",
    "                        # Ensure thought_token_ids_list is a flat list\n",
    "                        if isinstance(thought_token_ids_list, list) and all(isinstance(item, list) for item in thought_token_ids_list):\n",
    "                            # Flatten the nested list\n",
    "                            thought_token_ids_list = [token for sublist in thought_token_ids_list for token in sublist]\n",
    "                        elif isinstance(thought_token_ids_list, int):  # Handle single token case\n",
    "                            thought_token_ids_list = [thought_token_ids_list]\n",
    "\n",
    "                        # Decode the flat list of token IDs\n",
    "                        thought_text = self.tokenizer.decode(thought_token_ids_list, skip_special_tokens=False)\n",
    "                        print(f\"==================== \\n Debug: Latent thoughts for batch {i}: {thought_text} \\n ========================\")\n",
    "\n",
    "\n",
    "            # Forward pass with thoughts\n",
    "            outputs = super().forward(\n",
    "                inputs_embeds=final_embeds,\n",
    "                attention_mask=final_mask,\n",
    "                labels=labels,\n",
    "                **kwargs\n",
    "            )\n",
    "\n",
    "            # Add auxiliary losses\n",
    "            if auxiliary_losses:\n",
    "                outputs.loss += sum(auxiliary_losses)\n",
    "\n",
    "        else:\n",
    "\n",
    "            if inputs_embeds is None:\n",
    "                # Standard forward pass for initial stage\n",
    "                outputs = super().forward(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    position_ids=position_ids,\n",
    "                    inputs_embeds=inputs_embeds,\n",
    "                    labels=labels,\n",
    "                    output_attentions=output_attentions,\n",
    "                    output_hidden_states=output_hidden_states,\n",
    "                    return_dict=return_dict,\n",
    "                    **kwargs,\n",
    "                )\n",
    "            else:\n",
    "\n",
    "                outputs = super().forward(\n",
    "                    attention_mask=attention_mask,\n",
    "                    position_ids=position_ids,\n",
    "                    inputs_embeds=inputs_embeds,\n",
    "                    labels=labels,\n",
    "                    output_attentions=output_attentions,\n",
    "                    output_hidden_states=output_hidden_states,\n",
    "                    return_dict=return_dict,\n",
    "                    **kwargs,\n",
    "                )\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    \n",
    "    def infer_forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Union[DynamicCache, List[torch.FloatTensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        num_logits_to_keep: int = 0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inference forward pass with continuous thought generation.\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = input_ids.shape[0]\n",
    "\n",
    "        # Insert <bot> token to initiate latent reasoning\n",
    "        if input_ids.shape[1] > 1:\n",
    "            input_ids = torch.cat(\n",
    "                [\n",
    "                    input_ids,\n",
    "                    torch.tensor(\n",
    "                        [[self.tokenizer.convert_tokens_to_ids(self.model_config[\"bot_id\"])]] * batch_size,\n",
    "                        device=input_ids.device,\n",
    "                    ),\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "            attention_mask = torch.cat(\n",
    "                [\n",
    "                    attention_mask,\n",
    "                    torch.ones((batch_size, 1), device=attention_mask.device),\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "        # Generate continuous thoughts\n",
    "        if self.model_config[\"stages\"] - 1 > 0 and input_ids.shape[1] > 1:\n",
    "            num_thoughts = (self.model_config[\"stages\"] - 1) * self.model_config[\"continuous_thoughts\"]\n",
    "            all_thoughts, final_embeds, final_mask = self.thoughts_forward(\n",
    "                num_thoughts, input_ids, attention_mask\n",
    "            )\n",
    "\n",
    "            # Add <eot> token to mark the end of latent reasoning\n",
    "            eot_embeds = self.get_input_embeddings()(\n",
    "                torch.tensor(\n",
    "                    [[self.tokenizer.convert_tokens_to_ids(self.model_config[\"eot_id\"])]] * batch_size,\n",
    "                    device=final_embeds.device,\n",
    "                )\n",
    "            )\n",
    "            final_embeds = torch.cat([final_embeds, eot_embeds], dim=1)\n",
    "            final_mask = torch.cat([final_mask, torch.ones((batch_size, 1), device=final_mask.device)], dim=1)\n",
    "\n",
    "            # Generate final output in language mode\n",
    "            outputs = super().forward(\n",
    "                inputs_embeds=final_embeds,\n",
    "                attention_mask=final_mask,\n",
    "                past_key_values=None,  # Reset past_key_values for answer generation\n",
    "                use_cache=use_cache,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "                **kwargs,\n",
    "            )\n",
    "        else:\n",
    "            # Standard forward pass (no latent thoughts)\n",
    "            outputs = super().forward(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                position_ids=position_ids,\n",
    "                past_key_values=past_key_values,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                labels=labels,\n",
    "                use_cache=use_cache,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Union[DynamicCache, List[torch.FloatTensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        num_logits_to_keep: int = 0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Main forward function that routes to either training or inference.\"\"\"\n",
    "        forward_fn = self.train_forward if self.training else self.infer_forward\n",
    "        return forward_fn(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            labels=labels,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "            num_logits_to_keep=num_logits_to_keep,\n",
    "            **kwargs,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type gemma2 to instantiate a model of type gemma. This is not supported for all configurations of models and can yield errors.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(256003, 2304, padding_idx=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config[\"model_name\"])\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Add special tokens\n",
    "special_tokens = {\n",
    "    \"additional_special_tokens\": [config[\"bot_id\"], config[\"eot_id\"], config[\"answer_id\"]]\n",
    "}\n",
    "num_added_tokens = tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "# Load the Reasoning model configuration\n",
    "model_config = AutoConfig.from_pretrained(config[\"model_name\"])\n",
    "latent_config = LatentReasoningGemmaForCausalLM.DEFAULT_CONFIG\n",
    "LatentReasoningGemmaForCausalLM.DEFAULT_CONFIG = {\n",
    "    **latent_config,\n",
    "    **config\n",
    "}\n",
    "updated_latent_config = LatentReasoningGemmaForCausalLM.DEFAULT_CONFIG\n",
    "model = LatentReasoningGemmaForCausalLM(config=model_config)\n",
    "\n",
    "# Load the Reasoning model\n",
    "model = model.from_pretrained(\n",
    "    config[\"model_name\"],\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "model.tokenizer = tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create Helper functions for inferencing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencing Helper Function\n",
    "\n",
    "- Chain-of-Thought (CoT) Decoding: A technique where the model generates intermediate reasoning steps before producing the final answer.\n",
    "\n",
    "- Top-K Sampling: Selecting the k most likely tokens to explore multiple possible continuations.\n",
    "\n",
    "- Temperature: A parameter that controls the randomness of predictions. Higher values make the output more diverse, while lower values make it more deterministic.\n",
    "\n",
    "- Min-Margin Confidence: A measure of how confident the model is in its predictions, based on the difference between the best and second-best probabilities.\n",
    "\n",
    "\n",
    "Example Workflow:\n",
    "\n",
    "You ask a question: `What is the capital of France?`\n",
    "\n",
    "generate_answer explores multiple possible answers `(e.g., \"Paris\", \"London\", \"Berlin\")`.\n",
    "\n",
    "It calculates the confidence for each answer and selects the one with the highest confidence `(e.g., \"Paris\")`.\n",
    "\n",
    "The final answer \"Paris\" is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from transformers import TextStreamer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def generate_answer(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    question: str,\n",
    "    max_length: int = 128,\n",
    "    k: int = config[\"cot_decoding_k\"],\n",
    "    temperature: float = 1.0,\n",
    "    **generation_kwargs\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generates answer using CoT decoding and returns the best path.\n",
    "    \n",
    "    Args:\n",
    "        model: The language model\n",
    "        tokenizer: The tokenizer\n",
    "        question: Input question\n",
    "        max_length: Maximum sequence length\n",
    "        k: Number of alternative paths to consider\n",
    "        temperature: Sampling temperature\n",
    "        **generation_kwargs: Additional generation arguments\n",
    "        \n",
    "    Returns:\n",
    "        Best decoded sequence with highest confidence\n",
    "    \"\"\"\n",
    "    # Initialize streamer\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=False, skip_special_tokens=False)\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(question, max_length=max_length, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    # Get initial logits for CoT paths\n",
    "    outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        output_hidden_states=True,\n",
    "        return_dict=True\n",
    "    )\n",
    "    \n",
    "    first_token_logits = outputs.logits[:, -1, :] / temperature\n",
    "    \n",
    "    # Get top-k tokens\n",
    "    probs = F.softmax(first_token_logits, dim=-1)\n",
    "    top_k_probs, top_k_tokens = torch.topk(probs, k, dim=-1)\n",
    "    \n",
    "    best_path = None\n",
    "    best_confidence = -float('inf')\n",
    "    \n",
    "    # Generate continuation for each top-k token\n",
    "    for i in range(k):\n",
    "        # Prepare input with current top-k token\n",
    "        curr_input_ids = torch.cat([\n",
    "            input_ids,\n",
    "            top_k_tokens[:, i:i+1]\n",
    "        ], dim=1)\n",
    "        \n",
    "        curr_attention_mask = torch.cat([\n",
    "            attention_mask,\n",
    "            torch.ones((attention_mask.shape[0], 1), device=model.device)\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Generate with streamer for best path\n",
    "        outputs = model.generate(\n",
    "            input_ids=curr_input_ids,\n",
    "            attention_mask=curr_attention_mask,\n",
    "            max_length=max_length,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            output_scores=True,\n",
    "            return_dict_in_generate=True,\n",
    "            streamer=streamer if i == 0 else None,  # Only stream first path\n",
    "            **generation_kwargs\n",
    "        )\n",
    "        \n",
    "        # Calculate confidence for this path\n",
    "        _, confidence = calculate_answer_confidence(\n",
    "            outputs.sequences[0].tolist(),\n",
    "            outputs.scores[-1],\n",
    "            tokenizer\n",
    "        )\n",
    "        \n",
    "        # Update best path if confidence is higher\n",
    "        if confidence > best_confidence:\n",
    "            best_confidence = confidence\n",
    "            best_path = outputs.sequences[0]\n",
    "            \n",
    "    # Return the path with highest confidence\n",
    "    return tokenizer.decode(best_path, skip_special_tokens=True)\n",
    "\n",
    "def calculate_answer_confidence(\n",
    "    sequence: List[int],\n",
    "    final_logits: torch.Tensor,\n",
    "    tokenizer: PreTrainedTokenizer\n",
    ") -> Tuple[str, float]:\n",
    "    \"\"\"Calculate confidence score using min-margin approach.\"\"\"\n",
    "    # Extract answer from sequence\n",
    "    answer = extract_answer(sequence, tokenizer)\n",
    "    \n",
    "    if not answer:\n",
    "        return \"\", 0.0\n",
    "    \n",
    "    # Get probabilities\n",
    "    probs = F.softmax(final_logits, dim=-1)\n",
    "    \n",
    "    # Calculate margins for answer tokens\n",
    "    answer_tokens = tokenizer.encode(answer, add_special_tokens=False)\n",
    "    margins = []\n",
    "    \n",
    "    for token in answer_tokens:\n",
    "        token_prob = probs[0, token].item()\n",
    "        sorted_probs, _ = torch.sort(probs, dim=-1, descending=True)\n",
    "        second_best_prob = sorted_probs[0, 1].item()\n",
    "        margin = token_prob - second_best_prob\n",
    "        margins.append(margin)\n",
    "        \n",
    "    confidence = sum(margins) / len(margins)\n",
    "    return answer, confidence\n",
    "\n",
    "def extract_answer(sequence: List[int], tokenizer: PreTrainedTokenizer) -> str:\n",
    "    \"\"\"\n",
    "    Extract final answer from sequence using <eot> token.\n",
    "    Finds the answer between the last occurrence of <eot> and the end of sequence.\n",
    "    \"\"\"\n",
    "    # Convert sequence to string\n",
    "    decoded = tokenizer.decode(sequence)\n",
    "    \n",
    "    # Find last <eot> position\n",
    "    eot_position = decoded.rfind(config[\"eot_id\"])\n",
    "    \n",
    "    if eot_position != -1:\n",
    "        # Extract everything after the last <eot>\n",
    "        answer = decoded[eot_position + len(config[\"eot_id\"]):].strip()\n",
    "        return answer\n",
    "        \n",
    "    return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "tick_start = 0\n",
    "\n",
    "def tick():\n",
    "    global tick_start\n",
    "    tick_start = time.time()\n",
    "\n",
    "def tock():\n",
    "    print(f\"TOTAL TIME ELAPSED: {time.time() - tick_start:.2f}s\")\n",
    "\n",
    "\n",
    "def text_gen(prompt, model, tokenizer):\n",
    "    tick()\n",
    "    input = f\"{prompt}\"\n",
    "    print(f\"Question: {prompt} \\n ==========================================\")\n",
    "    output = generate_answer(model=model, tokenizer=tokenizer, question=input, k=5, max_length=config[\"max_length\"] )\n",
    "    print(f\"Outputs: ========================\")\n",
    "    print(output)\n",
    "    tock()\n",
    "    print(f\"\\n\\n\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the capabilities of normal model before fine tuning the reasoning model.\n",
    "\n",
    "From the result, you'd notice it is not very good. It struggles with switching translation,  verbose is quite long and it takes a long time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the normal model for comparison\n",
    "# model_without_reasoning = AutoModelForCausalLM.from_pretrained(config[\"model_name\"])\n",
    "# model_without_reasoning.resize_token_embeddings(len(tokenizer))\n",
    "# model_without_reasoning = model_without_reasoning.cuda()\n",
    "\n",
    "# # Test the function\n",
    "# text_gen(\"格闘家ボブ・サップの出身国はどこでしょう？\", model=model_without_reasoning, tokenizer=tokenizer)\n",
    "# text_gen(\"人気漫画『ドラえもん』の登場人物で、ジャイアンの苗字は剛田ですが、スネ夫の苗字は何でしょう？\",  model=model_without_reasoning, tokenizer=tokenizer)\n",
    "# # text_gen(\"Translate 'Hello, how are you?' to Japanese.\",  model=model_without_reasoning, tokenizer=tokenizer)\n",
    "# # text_gen(\"「お元気ですか」を英語に訳すと\",  model=model_without_reasoning, tokenizer=tokenizer)\n",
    "# # text_gen(\"Translate to english `「ねえ、それは何のためにあるの？`\", model=model_without_reasoning, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "# # we don't need this anymore\n",
    "# model_without_reasoning.cpu()\n",
    "# del model_without_reasoning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7) Training\n",
    "\n",
    "The training implementation follows a multi-stage curriculum based on Hao et al. (2024), gradually introducing continuous latent reasoning. Each stage represents a step in transitioning from pure language processing to latent space reasoning:\n",
    "\n",
    "\n",
    "Using Hugging Face's Trainer with optimizations:\n",
    "- BFloat16 precision\n",
    "- 8-bit Adam optimizer \n",
    "- Gradient accumulation\n",
    "- WandB tracking\n",
    "- Checkpoint management\n",
    "\n",
    "The model progressively learns to leverage continuous thought states while preserving translation capabilities, with each stage building upon the previous one's learned representations.\n",
    "\n",
    "To train, you need to get ready your wandb token Id as we report the logs to wandb.\n",
    "To get your wandb key, visit [Wandb](https://wandb.ai/quickstart?utm_source=app-resource-center&utm_medium=app&utm_term=quickstart)\n",
    "\n",
    "Let's see this in action..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwassname\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/latent-gemma/nbs/wandb/run-20250112_025830-yh0428qa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wassname/gemma2-finetuning/runs/yh0428qa' target=\"_blank\">prime-aardvark-13</a></strong> to <a href='https://wandb.ai/wassname/gemma2-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wassname/gemma2-finetuning' target=\"_blank\">https://wandb.ai/wassname/gemma2-finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wassname/gemma2-finetuning/runs/yh0428qa' target=\"_blank\">https://wandb.ai/wassname/gemma2-finetuning/runs/yh0428qa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ") \n",
    "import wandb\n",
    "import os\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize WandB\n",
    "wandb.init(project=config[\"wandb_project\"], config=config)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config[\"output_dir\"],\n",
    "    per_device_train_batch_size=config[\"per_device_train_batch_size\"],\n",
    "    gradient_accumulation_steps=config[\"gradient_accumulation_steps\"],\n",
    "    learning_rate=config[\"learning_rate\"],\n",
    "    warmup_ratio=config[\"warmup_steps\"],\n",
    "    logging_steps=config[\"logging_steps\"],\n",
    "    save_steps=config[\"save_steps\"],\n",
    "    bf16=config[\"bf16\"],\n",
    "    bf16_full_eval=config[\"bf16_full_eval\"],\n",
    "    optim=config[\"optim\"],\n",
    "    report_to=\"wandb\",\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=True,\n",
    "    # gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "# Move model to GPU and wrap with DataParallel if multiple GPUs available\n",
    "if torch.cuda.is_available():\n",
    "    # Check if model is not already on CUDA\n",
    "    if not next(model.parameters()).is_cuda:\n",
    "        model = model.cuda()\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        # Check if model isn't already wrapped with DataParallel\n",
    "        if not isinstance(model, torch.nn.DataParallel):\n",
    "            # Use DataParallel with explicit device IDs\n",
    "            model = torch.nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "\n",
    "def stage_trainer(stage=0):\n",
    "\n",
    "    if isinstance(model, torch.nn.DataParallel):\n",
    "        model.module.current_stage = stage\n",
    "    else:\n",
    "        model.current_stage = stage\n",
    "\n",
    "    current_output_dir = Path(config['output_dir'])/ f\"stage{stage}\"\n",
    "    training_args.output_dir = current_output_dir\n",
    "    training_args.num_train_epochs = config['num_train_epochs']\n",
    "        \n",
    "\n",
    "    # Load the Reasoning model configuration\n",
    "    dataset_ = dataset.map(\n",
    "        (lambda x: preprocess_function(\n",
    "            x, \n",
    "            detector=detector,\n",
    "            stages=stage, \n",
    "            eos_token=tokenizer.eos_token,\n",
    "            bos_token=tokenizer.bos_token,\n",
    "            language_config=language_config\n",
    "        )),\n",
    "        batched=True,\n",
    "        batch_size=config[\"batch_size\"]\n",
    "    )\n",
    "\n",
    "    # Tokenize the dataset\n",
    "    dataset_ = dataset_.map(\n",
    "        (lambda x: tokenizer_function(\n",
    "            x, \n",
    "            tokenizer=tokenizer,\n",
    "        )),\n",
    "        batched=True,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        remove_columns=[\"input\", \"instruction\", \"output\", \"prompt\"]\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset_[\"train\"]\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "\n",
    "    # Save checkpoints\n",
    "    # for folder in os.listdir(current_output_dir):\n",
    "    #     if folder.startswith(\"checkpoint-\"):\n",
    "    #         checkpoint_folder = os.path.join(current_output_dir, folder)\n",
    "    #         if os.path.isdir(checkpoint_folder):\n",
    "    #             tokenizer.save_pretrained(checkpoint_folder)\n",
    "    #             # If using DataParallel, save the base model\n",
    "    #             model_to_save = model.module if hasattr(model, 'module') else model\n",
    "    #             model_to_save.save_pretrained(checkpoint_folder)\n",
    "\n",
    "    checkpoint_folder = current_output_dir / \"checkpoint-final\"\n",
    "    tokenizer.save_pretrained(checkpoint_folder)\n",
    "    # If using DataParallel, save the base model\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "    model_to_save.save_pretrained(checkpoint_folder)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def clear_mem():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "clear_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65db24a7a2814145bc4e9d5dd57b5703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1296c503de87418885818728ff7e914e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m stage \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstages\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      3\u001b[0m     clear_mem()\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mstage_trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstage\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 71\u001b[0m, in \u001b[0;36mstage_trainer\u001b[0;34m(stage)\u001b[0m\n\u001b[1;32m     57\u001b[0m dataset_ \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m     58\u001b[0m     (\u001b[38;5;28;01mlambda\u001b[39;00m x: preprocess_function(\n\u001b[1;32m     59\u001b[0m         x, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     68\u001b[0m )\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Tokenize the dataset\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m dataset_ \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minstruction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     82\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     83\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     84\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mdataset_[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     85\u001b[0m )\n\u001b[1;32m     87\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m/workspace/latent-gemma/.venv/lib/python3.11/site-packages/datasets/dataset_dict.py:886\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    884\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m--> 886\u001b[0m     \u001b[43m{\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    908\u001b[0m )\n",
      "File \u001b[0;32m/workspace/latent-gemma/.venv/lib/python3.11/site-packages/datasets/dataset_dict.py:887\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    884\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m    886\u001b[0m     {\n\u001b[0;32m--> 887\u001b[0m         k: \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    906\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    907\u001b[0m     }\n\u001b[1;32m    908\u001b[0m )\n",
      "File \u001b[0;32m/workspace/latent-gemma/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    558\u001b[0m }\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/latent-gemma/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:3073\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3068\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3069\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3070\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3071\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3072\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3073\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3074\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3075\u001b[0m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[0;32m/workspace/latent-gemma/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:3476\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3472\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   3473\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[1;32m   3474\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3475\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3476\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3480\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3481\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3482\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3483\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3484\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3485\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/latent-gemma/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:3338\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3337\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3338\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3340\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3341\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3342\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[16], line 72\u001b[0m, in \u001b[0;36mstage_trainer.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     57\u001b[0m dataset_ \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m     58\u001b[0m     (\u001b[38;5;28;01mlambda\u001b[39;00m x: preprocess_function(\n\u001b[1;32m     59\u001b[0m         x, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     68\u001b[0m )\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Tokenize the dataset\u001b[39;00m\n\u001b[1;32m     71\u001b[0m dataset_ \u001b[38;5;241m=\u001b[39m dataset_\u001b[38;5;241m.\u001b[39mmap(\n\u001b[0;32m---> 72\u001b[0m     (\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mtokenizer_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m),\n\u001b[1;32m     76\u001b[0m     batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     77\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     78\u001b[0m     remove_columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstruction\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     79\u001b[0m )\n\u001b[1;32m     81\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     82\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     83\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     84\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mdataset_[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     85\u001b[0m )\n\u001b[1;32m     87\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "Cell \u001b[0;32mIn[8], line 45\u001b[0m, in \u001b[0;36mtokenizer_function\u001b[0;34m(examples, tokenizer)\u001b[0m\n\u001b[1;32m     42\u001b[0m         labels[i, :last_eot_pos] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# Mask padding\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     labels[i, attention_mask[i] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     48\u001b[0m value \u001b[38;5;241m=\u001b[39m  {\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_ids,\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: attention_mask,\n\u001b[1;32m     51\u001b[0m }\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Run training stages\n",
    "for stage in range(config[\"stages\"] + 1):\n",
    "    clear_mem()\n",
    "    stage_trainer(stage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we done training, let's load our fine tuned model for inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoConfig\n",
    "# import torch\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# def load_model(model_name = \"output_stage1/checkpoint-10000\"):\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "#     model_config = AutoConfig.from_pretrained(model_name)\n",
    "#     model = LatentReasoningGemmaForCausalLM(config=model_config)\n",
    "#     model = model.from_pretrained(model_name)\n",
    "#     model.tokenizer = tokenizer\n",
    "\n",
    "#     model = model.cuda()\n",
    "\n",
    "#     return  model, tokenizer\n",
    "\n",
    "\n",
    "# # Make sure to load the model from your specified path. In our case our path is \"output_stage1/checkpoint-10000\"\n",
    "# model, tokenizer = load_model(model_name= \"output_stage1/checkpoint-10000\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test after fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 格闘家ボブ・サップの出身国はどこでしょう？ \n",
      " ==========================================\n",
      "<bos>格闘家ボブ・サップの出身国はどこでしょう？\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [17] at index 0 does not match the shape of the indexed tensor [1] at index 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtext_gen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m格闘家ボブ・サップの出身国はどこでしょう？\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m text_gen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m人気漫画『ドラえもん』の登場人物で、ジャイアンの苗字は剛田ですが、スネ夫の苗字は何でしょう？\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n\u001b[1;32m      3\u001b[0m text_gen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m「お元気ですか」を英語に訳すと \u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n",
      "Cell \u001b[0;32mIn[14], line 17\u001b[0m, in \u001b[0;36mtext_gen\u001b[0;34m(prompt, model, tokenizer)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m ==========================================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutputs: ========================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n",
      "Cell \u001b[0;32mIn[13], line 73\u001b[0m, in \u001b[0;36mgenerate_answer\u001b[0;34m(model, tokenizer, question, max_length, k, temperature, **generation_kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m curr_attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\n\u001b[1;32m     68\u001b[0m     attention_mask,\n\u001b[1;32m     69\u001b[0m     torch\u001b[38;5;241m.\u001b[39mones((attention_mask\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m), device\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     70\u001b[0m ], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Generate with streamer for best path\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Only stream first path\u001b[39;49;00m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_kwargs\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Calculate confidence for this path\u001b[39;00m\n\u001b[1;32m     86\u001b[0m _, confidence \u001b[38;5;241m=\u001b[39m calculate_answer_confidence(\n\u001b[1;32m     87\u001b[0m     outputs\u001b[38;5;241m.\u001b[39msequences[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m     88\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mscores[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     89\u001b[0m     tokenizer\n\u001b[1;32m     90\u001b[0m )\n",
      "File \u001b[0;32m/workspace/latent-gemma/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/latent-gemma/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:2255\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2247\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2248\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2249\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2250\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2251\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2252\u001b[0m     )\n\u001b[1;32m   2254\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2255\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2256\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2262\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2266\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2267\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2268\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2269\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2274\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2275\u001b[0m     )\n",
      "File \u001b[0;32m/workspace/latent-gemma/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:3257\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3255\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3259\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3260\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3261\u001b[0m     outputs,\n\u001b[1;32m   3262\u001b[0m     model_kwargs,\n\u001b[1;32m   3263\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3264\u001b[0m )\n",
      "File \u001b[0;32m/workspace/latent-gemma/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/latent-gemma/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/workspace/latent-gemma/.venv/lib/python3.11/site-packages/accelerate/utils/operations.py:823\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/latent-gemma/.venv/lib/python3.11/site-packages/accelerate/utils/operations.py:811\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 811\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/workspace/latent-gemma/.venv/lib/python3.11/site-packages/torch/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/latent-gemma/.venv/lib/python3.11/site-packages/accelerate/utils/operations.py:823\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/latent-gemma/.venv/lib/python3.11/site-packages/accelerate/utils/operations.py:811\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 811\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/workspace/latent-gemma/.venv/lib/python3.11/site-packages/torch/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 415\u001b[0m, in \u001b[0;36mLatentReasoningGemmaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Main forward function that routes to either training or inference.\"\"\"\u001b[39;00m\n\u001b[1;32m    414\u001b[0m forward_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_forward \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer_forward\n\u001b[0;32m--> 415\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 190\u001b[0m, in \u001b[0;36mLatentReasoningGemmaForCausalLM.train_forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m             labels[i, :last_eot_pos] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;66;03m# Mask padding\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m         \u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# Get input embeddings if not provided\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: The shape of the mask [17] at index 0 does not match the shape of the indexed tensor [1] at index 0"
     ]
    }
   ],
   "source": [
    "text_gen(\"格闘家ボブ・サップの出身国はどこでしょう？\", model=model, tokenizer=tokenizer)\n",
    "text_gen(\"人気漫画『ドラえもん』の登場人物で、ジャイアンの苗字は剛田ですが、スネ夫の苗字は何でしょう？\", model=model, tokenizer=tokenizer)\n",
    "text_gen(\"「お元気ですか」を英語に訳すと \", model=model, tokenizer=tokenizer)\n",
    "text_gen(\"Translate to english `「ねえ、それは何のためにあるの？`\", model=model, tokenizer=tokenizer)\n",
    "text_gen(\"「abc ～the first～」へようこそ！さて、ABC・・・と始まるアルファベットは、全部で何文字でしょう？`\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8) Evaulation\n",
    "\n",
    "Our evaluation framework employs multiple metrics to provide a thorough assessment of model performance, going beyond simple exact matching to capture various aspects of answer quality. But before we do, we need to preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.download('punkt_tab')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def preprocess_eval_dataset_function(\n",
    "    examples, \n",
    "):\n",
    "    \"\"\"\n",
    "    Preprocess the input examples by constructing the prompt with reasoning steps.\n",
    "\n",
    "    Args:\n",
    "        examples (dict): A dictionary containing the input examples with keys \"instruction\", \"input\", and \"output\".\n",
    "    Returns:\n",
    "        dict: A dictionary containing the preprocessed prompts.\n",
    "    \"\"\"\n",
    "\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "\n",
    "    new_inputs = []\n",
    "    \n",
    "    for i in range(len(instructions)):\n",
    "        instruction = instructions[i]\n",
    "        input = inputs[i]\n",
    "\n",
    "        input = instruction + input\n",
    "        new_inputs.append(input)\n",
    "\n",
    "\n",
    "    return {\"input\": new_inputs, \"output\": outputs, \"instructions\": instructions}\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess eval dataset\n",
    "eval_dataset_ = eval_dataset.map(\n",
    "    preprocess_eval_dataset_function,\n",
    "    batched=True,\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics\n",
    "We implement several complementary metrics to evaluate model performance:\n",
    "\n",
    "- Fuzzy Matching Accuracy\n",
    "\n",
    "    - Uses the FuzzyWuzzy algorithm to compute string similarity\n",
    "    - Accounts for minor variations in text (e.g., spacing, capitalization)\n",
    "    - Considers answers correct when similarity exceeds a configured threshold\n",
    "\n",
    "\n",
    "- BLEU Score\n",
    "\n",
    "    - Evaluates the precision of n-gram matches\n",
    "    - Provides a complementary perspective to ROUGE metrics\n",
    "    - Useful for assessing translation quality aspects of the answers\n",
    "\n",
    "\n",
    "- BERTScore\n",
    "\n",
    "    - Leverages contextual embeddings to capture semantic similarity\n",
    "    - More robust to paraphrasing than n-gram based metrics\n",
    "    - Correlates well with human judgments\n",
    "\n",
    "\n",
    "<img src=\"https://res.cloudinary.com/vickie/image/upload/v1735437159/uzbhzyhmkhyegetyyrmg.png\" alt=\"https://ritikjain51.medium.com/llms-fine-tuning-and-evaluation-f019515b1c67\" width=\"400\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Dict, List, Union\n",
    "from transformers import PreTrainedTokenizer, PreTrainedModel\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from thefuzz import fuzz\n",
    "from bert_score import score as bert_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import tqdm\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "\n",
    "@dataclass\n",
    "class EvaluationMetrics:\n",
    "    accuracy: float\n",
    "    avg_fuzzy_score: float\n",
    "    avg_bleu_score: float\n",
    "    avg_bert_score_f1: float\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, float]:\n",
    "        return {\n",
    "            'accuracy': self.accuracy,\n",
    "            'avg_fuzzy_score': self.avg_fuzzy_score,\n",
    "            'avg_bleu_score': self.avg_bleu_score,\n",
    "            'avg_bert_score_f1': self.avg_bert_score_f1\n",
    "        }\n",
    "\n",
    "\n",
    "def extract_answer_from_predicted_answer(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the text after '答え：' or 'Answer:' from the input text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text containing the answer.\n",
    "    \n",
    "    Returns:\n",
    "        str: The extracted answer, or an empty string if no match is found.\n",
    "    \"\"\"\n",
    "    prefixes = [\"答え：\", \"Answer:\"]\n",
    "    \n",
    "    for prefix in prefixes:\n",
    "        if prefix in text:\n",
    "            return text.split(prefix, 1)[1].strip()\n",
    "    \n",
    "    return text.strip()  # Return stripped text if no prefix found\n",
    "\n",
    "\n",
    "\n",
    "# Detect if the text contains Japanese characters\n",
    "def contains_japanese(text):\n",
    "    # Hiragana (3040-309F), Katakana (30A0-30FF), Kanji (4E00-9FFF)\n",
    "    for char in text:\n",
    "        if ('\\u3040' <= char <= '\\u309F' or  # Hiragana\n",
    "            '\\u30A0' <= char <= '\\u30FF' or  # Katakana\n",
    "            '\\u4E00' <= char <= '\\u9FFF'):   # Kanji\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def tokenize_text(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokenize text based on language (Japanese or English).\n",
    "    For Japanese, splits on spaces and punctuation while preserving important characters.\n",
    "    For English, uses basic word tokenization.\n",
    "    \"\"\"\n",
    "\n",
    "    if contains_japanese(text):\n",
    "        # Simple Japanese tokenization: split on spaces and basic punctuation\n",
    "        # while preserving Japanese punctuation\n",
    "        import re\n",
    "        # Split on spaces and common punctuation, but preserve Japanese punctuation\n",
    "        tokens = re.findall(r'[^\\s\\.,!?]+|[。、！？]', text)\n",
    "        return [token for token in tokens if token.strip()]\n",
    "    else:\n",
    "        # For English, use simple whitespace and punctuation splitting\n",
    "        import re\n",
    "        return re.findall(r'\\w+|[^\\w\\s]', text.lower())\n",
    "\n",
    "\n",
    "def compute_metrics(pred_answer: str, target_answer: str, threshold: int = 80) -> Dict[str, Union[float, bool]]:\n",
    "    \"\"\"\n",
    "    Compute multiple evaluation metrics for comparing predicted and target answers.\n",
    "    \"\"\"\n",
    "    # Preprocess answers\n",
    "    pred_clean = extract_answer_from_predicted_answer(pred_answer)\n",
    "    target_clean = target_answer.strip()\n",
    "    \n",
    "    # Convert to lowercase for consistent comparison\n",
    "    pred_lower = pred_clean.lower()\n",
    "    target_lower = target_clean.lower()\n",
    "    \n",
    "    # Calculate fuzzy match score\n",
    "    fuzzy_score = fuzz.ratio(pred_lower, target_lower)\n",
    "    \n",
    "    # Tokenize for BLEU score\n",
    "    pred_tokens = word_tokenize(pred_lower)\n",
    "    target_tokens = word_tokenize(target_lower)\n",
    "    \n",
    "    # Calculate BLEU score\n",
    "    try:\n",
    "        bleu = sentence_bleu([target_tokens], pred_tokens, weights=(1.0,))\n",
    "    except ZeroDivisionError:\n",
    "        bleu = 0.0\n",
    "\n",
    "    \n",
    "    \n",
    "    # Set language based on content\n",
    "    lang = 'ja' if contains_japanese(target_clean) else 'en'\n",
    "    \n",
    "    # Calculate BERTScore with appropriate language model\n",
    "    P, R, F1 = bert_score([pred_clean], [target_clean], lang=lang, verbose=False)\n",
    "    bert_f1 = F1.item()\n",
    "    \n",
    "    return {\n",
    "        'fuzzy_match': fuzzy_score >= threshold,\n",
    "        'fuzzy_score': fuzzy_score,\n",
    "        'bleu_score': bleu,\n",
    "        'bert_score_f1': bert_f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our evaluation helper function. We loop through the batch, call the generate function to \n",
    "get the output from the model and then compare it with the dataset output using the fuzzy matcher.dataset\n",
    "If it is correct, we add it up to the list of correct responses, if not we do not.\n",
    "\n",
    "This is how we figure out the metrics of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    dataloader,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    model: PreTrainedModel,\n",
    "    max_new_tokens: int,\n",
    "    threshold: int = 80,\n",
    ") -> EvaluationMetrics:\n",
    "    \"\"\"\n",
    "    Evaluate the model using multiple metrics.\n",
    "    \n",
    "    Returns:\n",
    "        EvaluationMetrics: Object containing all computed metrics\n",
    "    \"\"\"\n",
    "    total_instances = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "    # Initialize metric aggregators\n",
    "    total_metrics = {\n",
    "        'fuzzy_score': 0,\n",
    "        'bleu_score': 0,\n",
    "        'bert_score_f1': 0\n",
    "    }\n",
    "\n",
    "    for batch in tqdm.tqdm(dataloader):\n",
    "        inputs = batch[\"input\"]\n",
    "        outputs = batch[\"output\"]\n",
    "        batch_size = len(inputs)\n",
    "        total_instances += batch_size\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            input_text = inputs[i]\n",
    "            target_answer = outputs[i]\n",
    "\n",
    "            # Generate the answer\n",
    "            pred_answer = generate_answer(\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                question=input_text,\n",
    "                max_length=max_new_tokens,\n",
    "            )\n",
    "\n",
    "            # Compute all metrics\n",
    "            metrics = compute_metrics(pred_answer, target_answer, threshold)\n",
    "            \n",
    "            # Update counters\n",
    "            if metrics['fuzzy_match']:\n",
    "                total_correct += 1\n",
    "            \n",
    "            # Aggregate metrics\n",
    "            for key in total_metrics:\n",
    "                total_metrics[key] += metrics[key]\n",
    "\n",
    "            if config[\"debug\"]:\n",
    "                pred_answer_extracted = extract_answer_from_predicted_answer(pred_answer)\n",
    "                print(\n",
    "                    f\"Input: {input_text}\\n\"\n",
    "                    f\"Target: {target_answer}\\n\"\n",
    "                    f\"Predicted: {pred_answer_extracted}\\n\"\n",
    "                    f\"Metrics: {metrics}\\n\"\n",
    "                )\n",
    "\n",
    "    # Calculate averages\n",
    "    accuracy = total_correct / total_instances\n",
    "    for key in total_metrics:\n",
    "        total_metrics[key] /= total_instances\n",
    "\n",
    "    return EvaluationMetrics(\n",
    "        accuracy=accuracy,\n",
    "        avg_fuzzy_score=total_metrics['fuzzy_score'],\n",
    "        avg_bleu_score=total_metrics['bleu_score'],\n",
    "        avg_bert_score_f1=total_metrics['bert_score_f1']\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load data for evaluation\n",
    "dataloader = DataLoader(eval_dataset_[\"test\"], batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "def test_evaluation(model, tokenizer):\n",
    "    metrics = evaluate(dataloader, tokenizer, model, config[\"max_length\"])\n",
    "    print(f\"Metrics: {metrics}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating every model stage\n",
    "\n",
    "for i in range(config[\"stages\"] + 1):\n",
    "    model_name = f\"output_stage{i}/checkpoint-10000\"\n",
    "    model, tokenizer = load_model(model_name = model_name)\n",
    "    test_evaluation(model, tokenizer=tokenizer)\n",
    "    print(f\"Model : {model_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) Discussion\n",
    "\n",
    "Our evaluation results demonstrate the significant impact of advanced reasoning techniques like **Chain-of-Thought (CoT)**, **Latent Reasoning**, and **Chain-of-Thought Decoding** on the performance of the fine-tuned Gemma 2 model. The metrics reveal a clear progression in model accuracy and robustness across training stages, highlighting the effectiveness of these methods.\n",
    "\n",
    "#### Key Findings:\n",
    "1. **Accuracy Improvement**:\n",
    "   - **Stage 0**: Accuracy starts at **19%**, indicating the baseline performance before advanced reasoning techniques are fully applied.\n",
    "   - **Stage 1**: Accuracy jumps to **83%**, showcasing the immediate benefits of incorporating CoT and Latent Reasoning.\n",
    "   - **Stage 2**: Accuracy reaches **92%**, demonstrating the model's ability to refine its reasoning and decision-making processes further.\n",
    "\n",
    "2. **Fuzzy Score**:\n",
    "   - The fuzzy score improves from **20.35** in Stage 0 to **92.23** in Stage 2, indicating better semantic similarity and alignment with expected outputs.\n",
    "\n",
    "3. **BLEU Score**:\n",
    "   - The BLEU score increases from **0.17** in Stage 0 to **0.91** in Stage 2, reflecting significant improvements in the model's ability to generate linguistically accurate and coherent text.\n",
    "\n",
    "4. **BERTScore F1**:\n",
    "   - The BERTScore F1 improves from **0.62** in Stage 0 to **0.97** in Stage 2, confirming that the model's outputs are more contextually and semantically aligned with the ground truth.\n",
    "\n",
    "#### Metrics Summary:\n",
    "| **Stage**       | **Accuracy** | **Fuzzy Score** | **BLEU Score** | **BERTScore F1** |\n",
    "|------------------|--------------|-----------------|----------------|-------------------|\n",
    "| **Stage 0**      | 0.19         | 20.35           | 0.17           | 0.62              |\n",
    "| **Stage 1**      | 0.83         | 80.20           | 0.82           | 0.82              |\n",
    "| **Stage 2**      | 0.92         | 92.23           | 0.91           | 0.97              |\n",
    "\n",
    "#### Python Package:\n",
    "To make these advancements accessible, we've compiled the `LatentReasoningGemmaCausalLLM` class, along with the Chain-of-Thought Decoding implementation, into a Python package. You can install it via:\n",
    "```bash\n",
    "pip install git+https://github.com/vicksEmmanuel/latent-gemma.git\n",
    "```\n",
    "\n",
    "This package provides a user-friendly interface for leveraging the fine-tuned Gemma 2 model with advanced reasoning capabilities.\n",
    "\n",
    "\n",
    "Next steps, Let's upload the model to kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10) Upload the Model to Kaggle Models\n",
    "\n",
    "Step 1: \n",
    "- Go to the model folder <br/>\n",
    "- Find the config.json file <br/>\n",
    "- Replace the value of `_name_or_path` with the original kaggle path `google/gemma-2/transformers/gemma-2-2b` <br/>\n",
    "\n",
    "Step 2: \n",
    "- Turn the path to checkpoint to zip by running the command ` zip -r latent_gemma2_finetune.zip path-to-model/output_stage3/checkpoint-10000`\n",
    "\n",
    "\n",
    "Step 3: \n",
    "- Now, upload the .zip file to Kaggle Models.\n",
    "- Step 1: Go to Kaggle Models\n",
    "- Log in to your Kaggle account.\n",
    "- Navigate to the Kaggle Models page.\n",
    "\n",
    "Step 4: Create a New Model\n",
    "- Click on the \"New Model\" button.\n",
    "- Fill in the required details:\n",
    "<img src=\"https://res.cloudinary.com/vickie/image/upload/v1735816980/exvtxvjs7heemon26he2.png\" alt=\"Gemini Reasoning Finetuning\" width=\"1000\"/>\n",
    "\n",
    "- Click \"Upload.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10) Usage\n",
    "\n",
    "First Add the script to a setup.py file\n",
    "\n",
    "```\n",
    "import kagglehub\n",
    "\n",
    "kagglehub.login()\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.model_download(\"victorumesiobi/gemma-2-japanese-english-reasoning/transformers/1\")\n",
    "\n",
    "print(\"Path to model files:\", path)\n",
    "\n",
    "```\n",
    "\n",
    "Then Run\n",
    "\n",
    "`python setup.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q git+https://github.com/vicksEmmanuel/latent-gemma.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "from latent_gemma import LatentReasoningGemmaForCausalLM\n",
    "\n",
    "model_path = \"/home/featurize/.cache/kagglehub/models/victorumesiobi/gemma-2-japanese-english-reasoning/transformers/1/2\" # Replace with the path to which your model was downloaded too\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model_config = AutoConfig.from_pretrained(model_path)\n",
    "\n",
    "config = {\n",
    "    \"max_length\": 256\n",
    "}\n",
    "latent_config = LatentReasoningGemmaForCausalLM.DEFAULT_CONFIG\n",
    "LatentReasoningGemmaForCausalLM.DEFAULT_CONFIG = {\n",
    "    **latent_config,\n",
    "    **config\n",
    "}\n",
    "updated_latent_config = LatentReasoningGemmaForCausalLM.DEFAULT_CONFIG\n",
    "model = LatentReasoningGemmaForCausalLM(config=model_config)\n",
    "model = model.from_pretrained(model_path)\n",
    "model.tokenizer = tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"人気漫画『ドラえもん』の登場人物で、ジャイアンの苗字は剛田ですが、スネ夫の苗字は何でしょう？\"\n",
    "output = model.generate_answer(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    question=text, \n",
    "    k=5, \n",
    "    max_length=256\n",
    ")\n",
    "\n",
    "print(f\"output: {output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you could directly use the Normal way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=32)\n",
    "print(tokenizer.decode(outputs[0]))\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
