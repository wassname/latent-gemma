{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- @format -->\n",
    "\n",
    "### 1) How to Fine-tune Gemma 2 for Advanced Reasoning in Communication, Translation and Multilingual Tasks\n",
    "\n",
    "Large Language Models (LLMs) like Gemma 2 have shown remarkable capabilities, but they can struggle with complex translation and cross-lingual communication tasks that require nuanced reasoning. Traditional fine-tuning with Chain-of-Thought (CoT) provides some improvements but has inherent limitations when dealing with multilingual scenarios.\n",
    "\n",
    "In this tutorial, we'll explore an innovative approach to enhance Gemma 2's reasoning capabilities by implementing the Coconut (Chain of Continuous Thought) paradigm introduced by [Hao et al. (2024)](https://arxiv.org/pdf/2412.06769). Instead of constraining the model to reason in language space, we'll leverage continuous latent representations to enable more flexible and powerful reasoning patterns, particularly beneficial for translation and cross-lingual tasks.\n",
    "\n",
    "By utilizing the last hidden state as a \"continuous thought\" and feeding it back directly as input embeddings, we can help the model develop more sophisticated reasoning strategies. This approach allows Gemma 2 to:\n",
    "\n",
    "- Explore multiple reasoning paths simultaneously\n",
    "- Avoid premature commitment to single translations\n",
    "- Handle complex linguistic nuances more effectively\n",
    "- Reduce token overhead during inference\n",
    "\n",
    "This tutorial will guide you through implementing this cutting-edge fine-tuning approach using a real-world dataset, helping you transform Gemma 2 into a more capable multilingual reasoning system.\n",
    "\n",
    "<img src=\"https://res.cloudinary.com/vickie/image/upload/v1735371553/ogylhcgz3o8trjtnjcov.png\" alt=\"Gemini Reasoning Finetuning\" width=\"1000\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some key concepts you need to know to better grasp the ideas of this tutorial\n",
    "\n",
    "##### <strong>Key Concepts</strong>\n",
    "<strong>Language Space</strong>: This is the discrete, symbolic representation of language (words, sentences).<br/>\n",
    "<strong>Continuous Thought Space (Latent Space)</strong>: This is the High-dimensional, continuous internal representations used by models for reasoning.\n",
    "\n",
    "###### <strong> Why Continuous Thought Space?</strong>\n",
    "<strong>Flexibility</strong>: Explore multiple reasoning paths simultaneously.<br/>\n",
    "<strong>Efficiency</strong>: Reduces token overhead by avoiding intermediate text. <br/>\n",
    "<strong>Nuance</strong>: Better captures complex linguistic and cross-lingual relationships.\n",
    "\n",
    "\n",
    "Having said this, let's begin by setting up the necessary environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Get Access to Gemma 2\n",
    "\n",
    "To get access to Gemma 2, follow these steps:\n",
    "\n",
    "1. Sign in or register on [Kaggle](https://kaggle.com)\n",
    "\n",
    "2. Visit the [Gemma 2 Model Card](https://www.kaggle.com/models/google/gemma/frameworks/transformers) \n",
    "\n",
    "3. Accept the terms and conditions to gain access to the model\n",
    "\n",
    "4. Once approved, you can use the model in your Kaggle notebooks or download locally\n",
    "\n",
    "Note: The Gemma 2 family consists of different model sizes (2B and 7B parameters). For this tutorial, we'll be using the 2B parameter version to make the fine-tuning process more manageable on typical hardware configurations. Speaking of hardware configuration: For this tutorial, we tested on RTX A6000, L20 and L40. This tutorial can also run on other GPUs, but if you get error similar to `CUDA memory limit` then use a different GPU\n",
    "\n",
    "\n",
    "Then run this command\n",
    "`pip install kagglehub`\n",
    "\n",
    "Copy the code below, put it in a python script `setup.py` and run python setup.py. The code is supposed to download the Gemma model to a local path and prints that local path.\n",
    "\n",
    "```\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "kagglehub.login()\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.model_download(\"google/gemma-2/transformers/gemma-2-2b\")\n",
    "\n",
    "print(\"Path to model files:\", path)\n",
    "\n",
    "```\n",
    "\n",
    "Copy the `path` printed and replace it at the 3) [Configuration `path`](#3-configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# CUDA_DEVICE_ORDER=1 CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # Check the number of available CUDA devices\n",
    "# num_devices = torch.cuda.device_count()\n",
    "# print(f\"Number of available CUDA devices: {num_devices}\")\n",
    "\n",
    "# # Print the name of each CUDA device\n",
    "# for i in range(num_devices):\n",
    "#     print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q -U wandb nltk rouge-score thefuzz python-Levenshtein bert-score evaluate transformers peft datasets janome numpy fuzzywuzzy bitsandbytes ml_dtypes tf_keras torch torchvision pytorch-lightning tensorflow scikit-learn tokenizers==0.20.1 huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"unsloth/gemma-2-2b\" # Make sure to change this to the path where the model is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "\n",
    "    # Core Learning Parameters\n",
    "    \"learning_rate\": 5e-5,                  # How fast the model learns (0.00005)\n",
    "    \"continuous_thoughts\": 4,               # Number of latent space reasoning steps\n",
    "    \"stages\": 4,                            # Number of training curriculum stages\n",
    "    \"training_thoughts_sequence_length\": 50, # Number of thought sequence to generate\n",
    "\n",
    "    # Inference and Evaluation Params       \n",
    "    \"fuzzy_matcher_threshold\": 80,          # Fuzzy matcher threshold at 80%\n",
    "    \"cot_decoding_k\": 5,                    # Number of paths to try before finding the best answer\n",
    "\n",
    "    # Model Setup\n",
    "    \"max_length\": 256,                      # Maximum text length to process\n",
    "    \"model_name\": path,                     # Path to Gemma model\n",
    "    \"batch_size\": 4,                        # Number of examples processed together\n",
    "    \"weight_decay\": 0.01,                   # Helps prevent overfitting\n",
    "\n",
    "    # Special Tokens\n",
    "    \"bot_id\": \"<bot>\",                      # Marks start of latent reasoning\n",
    "    \"eot_id\": \"<eot>\",                      # Marks end of latent reasoning\n",
    "    \"answer_id\": \"<answer>\",                # Marks the begining of answer\n",
    "    \"debug\": True,                          # Enables debugging output. Also allows you see the model's thoughts\n",
    "\n",
    "    # Training Optimizations\n",
    "    \"bf16\": True,                           # Uses BFloat16 for faster training\n",
    "    \"per_device_train_batch_size\": 1,       # Samples per GPU/CPU\n",
    "    \"optim\": \"adamw_torch\",                 # AdamW optimizer for efficiency\n",
    "    \"wandb_project\": \"gemma2-finetuning\",   # Tracks training on Weights & Biases\n",
    "    \"logging_steps\": 1,                     # How often to log training progress\n",
    "    \"bf16_full_eval\": True,                 # Uses BFloat16 for evaluation\n",
    "    \"gradient_accumulation_steps\": 1,       # How often to update weights\n",
    "    \"save_steps\": 10000,                    # How often to save model\n",
    "    \"warmup_steps\": 0.1,                    # Number of warmup steps\n",
    "    \"output_dir\": \"output\",                 # Where to save model files\n",
    "    \"diversity_weight\": 0.1,                # Reasoning diversity weight\n",
    "    \"coherence_weight\": 0.1                 # Reasoning coherence weight\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Dataset Overview: Japanese-English Translation/Communication\n",
    "<i>Using the llm-japanese-dataset created by [Hirano et al. (2023)](https://arxiv.org/pdf/2305.12720)</i>\n",
    "\n",
    "We'll be using the llm-japanese-dataset (8.4M records) to fine-tune Gemma 2, focusing on Japanese-English translation and communication. The dataset follows this format:\n",
    "\n",
    "```\n",
    "### Instruction:\n",
    "Please translate to English.\n",
    "\n",
    "### Input:\n",
    "こんにちは、元気ですか？\n",
    "\n",
    "### Response:\n",
    "Hello, how are you?\n",
    "```\n",
    "\n",
    "Most of the data (about 80%) consists of translations like this, making it perfect for improving Gemma 2's Japanese-English capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T18:12:09.715221Z",
     "iopub.status.busy": "2024-12-16T18:12:09.714341Z",
     "iopub.status.idle": "2024-12-16T18:13:04.760441Z",
     "shell.execute_reply": "2024-12-16T18:13:04.759432Z",
     "shell.execute_reply.started": "2024-12-16T18:12:09.715185Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from datasets import config as dataset_config\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "dataset_name = \"izumi-lab/llm-japanese-dataset\"\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# For this tutorial, let's take 30,000k samples from the dataset\n",
    "item = 30000\n",
    "\n",
    "truncated_dataset = DatasetDict({\n",
    "    split: dataset[split].select(range(item))\n",
    "    for split in dataset.keys()\n",
    "})\n",
    "\n",
    "\n",
    "dataset = truncated_dataset\n",
    "eval_dataset = dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see few examples of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T18:13:04.790751Z",
     "iopub.status.busy": "2024-12-16T18:13:04.790424Z",
     "iopub.status.idle": "2024-12-16T18:13:04.805684Z",
     "shell.execute_reply": "2024-12-16T18:13:04.804873Z",
     "shell.execute_reply.started": "2024-12-16T18:13:04.790716Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:  「abc ～the first～」へようこそ！さて、ABC・・・と始まるアルファベットは、全部で何文字でしょう？ \n",
      "\n",
      "Input:   \n",
      "\n",
      "Output:  26文字 \n",
      "\n",
      "========================================================================================================================================================================================================\n",
      "\n",
      "Instruction:  人気漫画『ドラえもん』の登場人物で、ジャイアンの苗字は剛田ですが、スネ夫の苗字は何でしょう？ \n",
      "\n",
      "Input:   \n",
      "\n",
      "Output:  骨川（滑川も正解） \n",
      "\n",
      "========================================================================================================================================================================================================\n",
      "\n",
      "Instruction:  格闘家ボブ・サップの出身国はどこでしょう？ \n",
      "\n",
      "Input:   \n",
      "\n",
      "Output:  アメリカ \n",
      "\n",
      "========================================================================================================================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(\"Instruction: \", dataset['train'][\"instruction\"][i], \"\\n\")\n",
    "    print(\"Input: \", dataset['train'][\"input\"][i], \"\\n\")\n",
    "    print(\"Output: \",dataset['train'][\"output\"][i], \"\\n\")\n",
    "    print(f\"{'='*200}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5) Language Detector\n",
    "\n",
    "When handling multilingual content, detecting languages accurately is crucial. Our language detector analyzes both the structure and composition of mixed-language text.\n",
    "\n",
    "For example, let's examine this input:\n",
    "```python\n",
    "\"「abc ～the first～」へようこそ！さて、ABC・・・と始まるアルファベットは、全部で何文字でしょう？\"\n",
    "```\n",
    "\n",
    "The detector identifies Japanese as the primary language while recognizing English phrases (abc, first, ABC) embedded within. It analyzes the distribution of scripts including Hiragana, Latin alphabet, and various symbols. This composition analysis helps determine language percentages and structure.\n",
    "\n",
    "Beyond basic detection, this analysis guides translation strategy and validates output language alignment. By understanding the input language composition, Gemma 2 can better reason about how to process and generate appropriate bilingual responses. The detector becomes especially valuable when building chain-of-thought reasoning patterns across languages.\n",
    "\n",
    "Let's implement this detector..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T18:15:41.502874Z",
     "iopub.status.busy": "2024-12-16T18:15:41.502091Z",
     "iopub.status.idle": "2024-12-16T18:15:41.517010Z",
     "shell.execute_reply": "2024-12-16T18:15:41.515979Z",
     "shell.execute_reply.started": "2024-12-16T18:15:41.502815Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: スナフキン ===>>>> {'Japanese': 100.0}\n",
      "Text: レベッカ(REBECCA) ===>>>> {'English': 63.6, 'Japanese': 36.4}\n",
      "Text: Hello World ===>>>> {'English': 100.0}\n",
      "Text: こんにちは World! ===>>>> {'Japanese': 50.0, 'English': 50.0}\n"
     ]
    }
   ],
   "source": [
    "# Implementing the LanguageDetector class\n",
    "\n",
    "from typing import Dict, List, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ScriptRange:\n",
    "    \"\"\"Represents a Unicode range for a writing system\"\"\"\n",
    "    start: int\n",
    "    end: int\n",
    "    name: str\n",
    "    \n",
    "class LanguageDetector:\n",
    "    def __init__(self):\n",
    "        self.scripts: List[ScriptRange] = []\n",
    "        self.language_mappings: Dict[str, List[str]] = {}\n",
    "        \n",
    "    def add_script(self, name: str, start: int, end: int) -> None:\n",
    "        \"\"\"\n",
    "        Add a new script range to the detector\n",
    "        \n",
    "        Args:\n",
    "            name: Name of the script (e.g., 'Hiragana', 'Latin')\n",
    "            start: Starting Unicode code point\n",
    "            end: Ending Unicode code point\n",
    "        \"\"\"\n",
    "        self.scripts.append(ScriptRange(start, end, name))\n",
    "    \n",
    "    def map_scripts_to_language(self, language: str, script_names: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Map multiple scripts to a single language\n",
    "        \n",
    "        Args:\n",
    "            language: Name of the language (e.g., 'Japanese')\n",
    "            script_names: List of script names that belong to this language\n",
    "        \"\"\"\n",
    "        self.language_mappings[language] = script_names\n",
    "    \n",
    "    def detect(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Detect the percentage of different languages/scripts in the text\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to analyze\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping language/script names to their percentage presence\n",
    "        \"\"\"\n",
    "        # Count characters in each script\n",
    "        char_counts: Dict[str, int] = {script.name: 0 for script in self.scripts}\n",
    "        total_chars = 0\n",
    "        \n",
    "        for char in text:\n",
    "            if char.isspace() or char in '.,!?()[]{}':\n",
    "                continue\n",
    "                \n",
    "            code = ord(char)\n",
    "            total_chars += 1\n",
    "            \n",
    "            # Check which script range the character falls into\n",
    "            for script in self.scripts:\n",
    "                if script.start <= code <= script.end:\n",
    "                    char_counts[script.name] += 1\n",
    "                    break\n",
    "        \n",
    "        if total_chars == 0:\n",
    "            return {}\n",
    "            \n",
    "        # Calculate initial percentages\n",
    "        percentages = {\n",
    "            script: (count / total_chars) * 100\n",
    "            for script, count in char_counts.items()\n",
    "            if count > 0\n",
    "        }\n",
    "        \n",
    "        # Combine scripts into languages where applicable\n",
    "        final_percentages = {}\n",
    "        used_scripts = set()\n",
    "        \n",
    "        # First, handle mapped languages\n",
    "        for language, script_names in self.language_mappings.items():\n",
    "            total = sum(percentages.get(script, 0) for script in script_names)\n",
    "            if total > 0:\n",
    "                final_percentages[language] = total\n",
    "                used_scripts.update(script_names)\n",
    "        \n",
    "        # Then add remaining unmapped scripts\n",
    "        for script, percentage in percentages.items():\n",
    "            if script not in used_scripts:\n",
    "                final_percentages[script] = percentage\n",
    "        \n",
    "        return {k: round(v, 1) for k, v in sorted(\n",
    "            final_percentages.items(),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )}\n",
    "\n",
    "# Example setup and usage\n",
    "def create_default_detector() -> LanguageDetector:\n",
    "    \"\"\"Create a detector with Japanese and English support\"\"\"\n",
    "    detector = LanguageDetector()\n",
    "    \n",
    "    # Add Japanese scripts\n",
    "    detector.add_script('Hiragana', 0x3040, 0x309F)\n",
    "    detector.add_script('Katakana', 0x30A0, 0x30FF)\n",
    "    detector.add_script('Kanji', 0x4E00, 0x9FFF)\n",
    "\n",
    "    # Add English scripts\n",
    "    detector.add_script('Latin', 0x0000, 0x024F)\n",
    "\n",
    "    \n",
    "    # Map scripts to languages\n",
    "    detector.map_scripts_to_language('Japanese', ['Hiragana', 'Katakana', 'Kanji'])\n",
    "    detector.map_scripts_to_language('English', ['Latin'])\n",
    "    \n",
    "    return detector\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    detector = create_default_detector()\n",
    "    \n",
    "    test_texts = [\n",
    "        'スナフキン',\n",
    "        'レベッカ(REBECCA)',\n",
    "        'Hello World',\n",
    "        'こんにちは World!'\n",
    "    ]\n",
    "    \n",
    "    for text in test_texts:\n",
    "        result = detector.detect(text)\n",
    "        print(f\"Text: {text} ===>>>> {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6) Dataset Preprocessing\n",
    "\n",
    "The preprocessing pipeline prepares our dataset for Continuous Latent Reasoning by transforming raw translation pairs into structured training data. Here's what each component does:\n",
    "\n",
    "`preprocess_function`: \n",
    "Takes raw examples and generates Chain-of-Thought reasoning steps. For each sample, it:\n",
    "1. Analyzes language composition of input/output\n",
    "2. Generates appropriate reasoning steps in detected language\n",
    "3. Formats with special tokens (bot_token, eot_token) as per Hao et al. (2024)\n",
    "\n",
    "Example flow:\n",
    "```python\n",
    "Input: \"「abc ～the first～」へようこそ！\"\n",
    "Steps:\n",
    "- Detect languages (Japanese: 60%, English: 40%)\n",
    "- Generate understanding steps\n",
    "- Format with special tokens\n",
    "Output: \"<bos> Input <eos><bot><eot> Step 1... Step 2... Answer <eos>\"\n",
    "```\n",
    "\n",
    "`tokenizer_function`:\n",
    "Converts text into model inputs by:\n",
    "1. Tokenizing the formatted text\n",
    "2. Creating attention masks\n",
    "3. Preparing labels (masking question/thought tokens)\n",
    "\n",
    "\n",
    "This preprocessing ensures our data is properly structured for training Gemma 2 in continuous latent space reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "def preprocess_function(\n",
    "    examples, \n",
    "    detector=None,  # Make detector optional\n",
    "    stages=1, \n",
    "    eos_token=\"<eos>\",\n",
    "    bos_token=\"<bos>\",\n",
    "    language_config=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Preprocess the input examples by constructing the prompt with reasoning steps.\n",
    "\n",
    "    Args:\n",
    "        examples (dict): A dictionary containing the input examples with keys \"instruction\", \"input\", and \"output\".\n",
    "        detector: A language detection object or function that detects the language of a given text.\n",
    "        stages (int): The number of reasoning stages to include in the prompt.\n",
    "        eos_token (str): The end-of-sequence token.\n",
    "        bos_token (str): The beginning-of-sequence token.\n",
    "        language_config (dict): A dictionary mapping language keys to their respective translations for steps and labels.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the preprocessed prompts.\n",
    "    \"\"\"\n",
    "\n",
    "    if language_config is None:\n",
    "        language_config = {\n",
    "            \"English\": {\n",
    "                \"language_detection\": \"Question language detection\",\n",
    "                \"understand_question\": \"Understand the question\",\n",
    "                \"understand_answer\": \"Understand the answer\",\n",
    "                \"response_language_detection\": \"Response language detection\",\n",
    "                \"answer_label\": \"Answer:\",\n",
    "                \"step_label\": \"Step\",\n",
    "            },\n",
    "            \"Japanese\": {\n",
    "                \"language_detection\": \"言語の検出\",\n",
    "                \"understand_question\": \"質問を理解する\",\n",
    "                \"understand_answer\": \"答えを理解する\",\n",
    "                \"response_language_detection\": \"応答言語の検出\",\n",
    "                \"answer_label\": \"答え：\",\n",
    "                \"step_label\": \"ステップ\",\n",
    "            },\n",
    "            # Add more languages here as needed\n",
    "        }\n",
    "\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "\n",
    "    bot = config[\"bot_id\"]\n",
    "    eot = config[\"eot_id\"]\n",
    "    answer_token = config[\"answer_id\"]\n",
    "\n",
    "    # Initialize output dictionaries with lists\n",
    "    result = []\n",
    "\n",
    "    for i in range(len(instructions)):\n",
    "        instruction = instructions[i]\n",
    "        input = inputs[i]\n",
    "        output = outputs[i]\n",
    "\n",
    "        if len(input) > 1:\n",
    "            input = instruction + input\n",
    "        else:\n",
    "            input = instruction\n",
    "\n",
    "        # Use the provided detector to detect languages\n",
    "        input_language = detector.detect(input) if detector else {\"English\": 100.0}  # Default to English if no detector\n",
    "        output_language = detector.detect(output) if detector else {\"English\": 100.0}  # Default to English if no detector\n",
    "\n",
    "        steps = []\n",
    "\n",
    "        # Determine the primary input and output languages\n",
    "        # Use the language key from the detector's output that matches a key in language_config\n",
    "        input_lang = next((lang for lang in input_language if lang in language_config), \"English\")\n",
    "        output_lang = next((lang for lang in output_language if lang in language_config), \"English\")\n",
    "\n",
    "        # Get the language-specific labels\n",
    "        input_labels = language_config.get(input_lang, language_config[\"English\"])\n",
    "        output_labels = language_config.get(output_lang, language_config[\"English\"])\n",
    "\n",
    "        # Input language detection\n",
    "        input_lang_str = \", \".join([f\"{k}: {v}%\" for k, v in input_language.items()])\n",
    "        steps.append(f\"{input_labels['language_detection']}: {input_lang_str}\")\n",
    "        steps.append(f\"{input_labels['understand_question']}: {input}\")\n",
    "        steps.append(f\"{input_labels['understand_answer']}: {output}\")\n",
    "\n",
    "        # Output language detection\n",
    "        output_lang_str = \", \".join([f\"{k}: {v}%\" for k, v in output_language.items()]) if output_language else \"Unknown\"\n",
    "        steps.append(f\"{output_labels['response_language_detection']}: {output_lang_str}\")\n",
    "\n",
    "        # Format steps with step numbers\n",
    "        steps = [f\"{output_labels['step_label']} {i+1} : {step}\" for i, step in enumerate(steps)]\n",
    "\n",
    "        # Include only the steps relevant to the current stage\n",
    "        if stages > 0:\n",
    "            steps = steps[-stages:]  # Keep the last `stages` steps\n",
    "\n",
    "        # Renumber steps to start from 1\n",
    "        steps = [f\"{output_labels['step_label']} {i+1} : {step.split(' : ')[1]}\" for i, step in enumerate(steps)]\n",
    "\n",
    "        # Construct the prompt\n",
    "        prompt = bos_token + \"\\n\" + input + eos_token + bot + eot + \"\\n\" + \"\\n\".join(steps) + \"\\n\" + answer_token + output_labels['answer_label'] + output + eos_token\n",
    "\n",
    "        result.append(prompt)\n",
    "\n",
    "    return {\n",
    "        \"prompt\": result\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def tokenizer_function(examples, tokenizer):\n",
    "    \"\"\"\n",
    "    Tokenize the input prompt and prepare the input_ids, attention_mask, and labels for training.\n",
    "\n",
    "    Args:\n",
    "        examples (dict): A dictionary containing the input prompts.\n",
    "        tokenizer (PreTrainedTokenizer): The tokenizer to use for tokenization.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the tokenized input_ids, attention_mask, and labels.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = examples[\"prompt\"]\n",
    "    eot = config[\"eot_id\"]\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        prompt,\n",
    "        max_length=config[\"max_length\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    input_ids = tokenized[\"input_ids\"].squeeze(0)\n",
    "    attention_mask = tokenized[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "    labels = input_ids.clone()\n",
    "    batch_size = labels.shape[0]\n",
    "    eot_id = tokenizer.convert_tokens_to_ids(eot)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Find the positions of <eot> in the input_ids\n",
    "        eot_pos = (input_ids[i] == eot_id).nonzero(as_tuple=True)\n",
    "\n",
    "        if len(eot_pos[0]) > 0:\n",
    "            # Get the last occurrence of <eot>\n",
    "            last_eot_pos = eot_pos[0][-1].item()\n",
    "            \n",
    "            # Mask everything before and including the last <eot>\n",
    "            labels[i, :last_eot_pos] = -100\n",
    "\n",
    "        # Mask padding\n",
    "        labels[i, attention_mask[i] == 0] = -100\n",
    "\n",
    "\n",
    "    value =  {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "    }\n",
    "\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        return value\n",
    "    else:\n",
    "        value[\"labels\"] = labels\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language Config. If you need to handle more languages, you can add to this configuration and pick a suitable language detector as `LanguageDetector` class only support `English` and `Japanese` ... Note the keys `English`, `Japanese` or any other you intend to include must match the response from your Language detector `Text: レベッカ(REBECCA) ===>>>> {'English': 63.6, 'Japanese': 36.4}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_config = {\n",
    "    \"English\": {\n",
    "        \"language_detection\": \"Question language detection\",\n",
    "        \"understand_question\": \"Understand the question\",\n",
    "        \"understand_answer\": \"Understand the answer\",\n",
    "        \"response_language_detection\": \"Response language detection\",\n",
    "        \"answer_label\": \"Answer:\",\n",
    "        \"step_label\": \"Step\",\n",
    "    },\n",
    "    \"Japanese\": {\n",
    "        \"language_detection\": \"言語の検出\",\n",
    "        \"understand_question\": \"質問を理解する\",\n",
    "        \"understand_answer\": \"答えを理解する\",\n",
    "        \"response_language_detection\": \"応答言語の検出\",\n",
    "        \"answer_label\": \"答え：\",\n",
    "        \"step_label\": \"ステップ\",\n",
    "    },\n",
    "    # Add more languages here as needed\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize what our dataset looks like preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage: ========================>>>>>>>>>>>>>>>>> 0\n",
      "Input:  <bos>\n",
      "「abc ～the first～」へようこそ！さて、ABC・・・と始まるアルファベットは、全部で何文字でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 59.3%, English: 25.9%\n",
      "ステップ 2 : 質問を理解する: 「abc ～the first～」へようこそ！さて、ABC・・・と始まるアルファベットは、全部で何文字でしょう？\n",
      "ステップ 3 : 答えを理解する: 26文字\n",
      "ステップ 4 : 応答言語の検出: Japanese: 50.0%, English: 50.0%\n",
      "<answer>答え：26文字<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "人気漫画『ドラえもん』の登場人物で、ジャイアンの苗字は剛田ですが、スネ夫の苗字は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 89.1%\n",
      "ステップ 2 : 質問を理解する: 人気漫画『ドラえもん』の登場人物で、ジャイアンの苗字は剛田ですが、スネ夫の苗字は何でしょう？\n",
      "ステップ 3 : 答えを理解する: 骨川（滑川も正解）\n",
      "ステップ 4 : 応答言語の検出: Japanese: 77.8%\n",
      "<answer>答え：骨川（滑川も正解）<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "格闘家ボブ・サップの出身国はどこでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 95.2%\n",
      "ステップ 2 : 質問を理解する: 格闘家ボブ・サップの出身国はどこでしょう？\n",
      "ステップ 3 : 答えを理解する: アメリカ\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：アメリカ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ロシア語で「城」という意味がある、ロシアの大統領府の別名は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 88.6%\n",
      "ステップ 2 : 質問を理解する: ロシア語で「城」という意味がある、ロシアの大統領府の別名は何でしょう？\n",
      "ステップ 3 : 答えを理解する: クレムリン\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：クレムリン<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "織田信長、豊臣秀吉、徳川家康という３人の戦国武将の性格を表現するのに用いられる鳥は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 言語の検出: Japanese: 91.5%\n",
      "ステップ 2 : 質問を理解する: 織田信長、豊臣秀吉、徳川家康という３人の戦国武将の性格を表現するのに用いられる鳥は何でしょう？\n",
      "ステップ 3 : 答えを理解する: ホトトギス\n",
      "ステップ 4 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ホトトギス<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Stage: ========================>>>>>>>>>>>>>>>>> 1\n",
      "Input:  <bos>\n",
      "「abc ～the first～」へようこそ！さて、ABC・・・と始まるアルファベットは、全部で何文字でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 50.0%, English: 50.0%\n",
      "<answer>答え：26文字<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "人気漫画『ドラえもん』の登場人物で、ジャイアンの苗字は剛田ですが、スネ夫の苗字は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 77.8%\n",
      "<answer>答え：骨川（滑川も正解）<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "格闘家ボブ・サップの出身国はどこでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：アメリカ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ロシア語で「城」という意味がある、ロシアの大統領府の別名は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：クレムリン<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "織田信長、豊臣秀吉、徳川家康という３人の戦国武将の性格を表現するのに用いられる鳥は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ホトトギス<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Stage: ========================>>>>>>>>>>>>>>>>> 2\n",
      "Input:  <bos>\n",
      "「abc ～the first～」へようこそ！さて、ABC・・・と始まるアルファベットは、全部で何文字でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 26文字\n",
      "ステップ 2 : 応答言語の検出: Japanese: 50.0%, English: 50.0%\n",
      "<answer>答え：26文字<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "人気漫画『ドラえもん』の登場人物で、ジャイアンの苗字は剛田ですが、スネ夫の苗字は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: 骨川（滑川も正解）\n",
      "ステップ 2 : 応答言語の検出: Japanese: 77.8%\n",
      "<answer>答え：骨川（滑川も正解）<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "格闘家ボブ・サップの出身国はどこでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: アメリカ\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：アメリカ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ロシア語で「城」という意味がある、ロシアの大統領府の別名は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: クレムリン\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：クレムリン<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "織田信長、豊臣秀吉、徳川家康という３人の戦国武将の性格を表現するのに用いられる鳥は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 答えを理解する: ホトトギス\n",
      "ステップ 2 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ホトトギス<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Stage: ========================>>>>>>>>>>>>>>>>> 3\n",
      "Input:  <bos>\n",
      "「abc ～the first～」へようこそ！さて、ABC・・・と始まるアルファベットは、全部で何文字でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 「abc ～the first～」へようこそ！さて、ABC・・・と始まるアルファベットは、全部で何文字でしょう？\n",
      "ステップ 2 : 答えを理解する: 26文字\n",
      "ステップ 3 : 応答言語の検出: Japanese: 50.0%, English: 50.0%\n",
      "<answer>答え：26文字<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "人気漫画『ドラえもん』の登場人物で、ジャイアンの苗字は剛田ですが、スネ夫の苗字は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 人気漫画『ドラえもん』の登場人物で、ジャイアンの苗字は剛田ですが、スネ夫の苗字は何でしょう？\n",
      "ステップ 2 : 答えを理解する: 骨川（滑川も正解）\n",
      "ステップ 3 : 応答言語の検出: Japanese: 77.8%\n",
      "<answer>答え：骨川（滑川も正解）<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "格闘家ボブ・サップの出身国はどこでしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 格闘家ボブ・サップの出身国はどこでしょう？\n",
      "ステップ 2 : 答えを理解する: アメリカ\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：アメリカ<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "ロシア語で「城」という意味がある、ロシアの大統領府の別名は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: ロシア語で「城」という意味がある、ロシアの大統領府の別名は何でしょう？\n",
      "ステップ 2 : 答えを理解する: クレムリン\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：クレムリン<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Input:  <bos>\n",
      "織田信長、豊臣秀吉、徳川家康という３人の戦国武将の性格を表現するのに用いられる鳥は何でしょう？<eos><bot><eot>\n",
      "ステップ 1 : 質問を理解する: 織田信長、豊臣秀吉、徳川家康という３人の戦国武将の性格を表現するのに用いられる鳥は何でしょう？\n",
      "ステップ 2 : 答えを理解する: ホトトギス\n",
      "ステップ 3 : 応答言語の検出: Japanese: 100.0%\n",
      "<answer>答え：ホトトギス<eos> \n",
      "\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# So we do not load every dataset as this takes a while\n",
    "truncated_dataset = DatasetDict({\n",
    "    split: dataset[split].select(range(5))\n",
    "    for split in dataset.keys()\n",
    "})\n",
    "\n",
    "for stage in range(config[\"stages\"]):\n",
    "    dataset_ = truncated_dataset.map(\n",
    "        (lambda x: preprocess_function(\n",
    "            x, \n",
    "            detector=detector,\n",
    "            stages=stage, \n",
    "            language_config=language_config\n",
    "        )),\n",
    "        batched=True,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "    )\n",
    "\n",
    "    print(f\"Stage: ========================>>>>>>>>>>>>>>>>> {stage}\")\n",
    "    for i in range(5):\n",
    "        print(\"Input: \", dataset_['train'][\"prompt\"][i], \"\\n\")\n",
    "        print(f\"{'='*100}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6) Modelling\n",
    "\n",
    "Our `LatentReasoningGemmaForCausalLM` extends GemmaForCausalLM to enable continuous latent reasoning, following the architecture from Hao et al. (2024). The model implements two key forward paths:\n",
    "\n",
    "`infer_forward`: During inference, transforms input text into continuous thought representations before generating the final output. It maintains a chain of latent states between the `<bot>` and `<eot>` tokens, allowing for more nuanced reasoning across languages.\n",
    "\n",
    "`train_forward`: During training, processes the sequence in stages, gradually building up continuous thought representations while masking appropriate parts of the input. It helps the model learn to reason in latent space while maintaining language understanding.\n",
    "\n",
    "Let's proceed with the creation of our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import GemmaForCausalLM, DynamicCache, PreTrainedTokenizer\n",
    "from typing import Optional, List, Union, Dict, Any\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LatentReasoningGemmaForCausalLM(GemmaForCausalLM):\n",
    "    \"\"\"\n",
    "    A custom implementation of GemmaForCausalLM that supports latent reasoning \n",
    "    using the Coconut (Chain of Continuous Thought) paradigm.\n",
    "    \"\"\"\n",
    "\n",
    "    DEFAULT_CONFIG = {\n",
    "        # Core Learning Parameters\n",
    "        \"continuous_thoughts\": 4,               # Number of latent space reasoning steps\n",
    "        \"stages\": 4,                            # Number of training curriculum stages\n",
    "        \"training_thoughts_sequence_length\": 50, # Number of thought sequence to generate\n",
    "\n",
    "        # Inference and Evaluation Params       \n",
    "        \"fuzzy_matcher_threshold\": 80,          # Fuzzy matcher threshold at 80%\n",
    "        \"cot_decoding_k\": 5,                    # Number of paths to try before finding the best answer\n",
    "\n",
    "        # Model Setup\n",
    "        \"max_length\": 256,                      # Maximum text length to process\n",
    "\n",
    "        # Special Tokens\n",
    "        \"bot_id\": \"<bot>\",                      # Marks start of latent reasoning\n",
    "        \"eot_id\": \"<eot>\",                      # Marks end of latent reasoning\n",
    "        \"answer_id\": \"<answer>\",                # Marks the begining of answer\n",
    "        \"debug\": True,                          # Enables debugging output. Also allows you see the model's thoughts\n",
    "\n",
    "    }\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.tokenizer: PreTrainedTokenizer = None\n",
    "        self.current_stage = 0\n",
    "        self.model_config = type(self).DEFAULT_CONFIG\n",
    "        self.debug = self.model_config.get(\"debug\", False)\n",
    "        self.diversity_weight = self.model_config.get(\"diversity_weight\", 0.1)\n",
    "        self.coherence_weight = self.model_config.get(\"coherence_weight\", 0.1)\n",
    "\n",
    "    def get_input_ids(self, inputs_embeds):\n",
    "        \"\"\"Helper method to get input ids from embeddings.\"\"\"\n",
    "        embedding_matrix = self.get_input_embeddings().weight\n",
    "        similarities = torch.matmul(inputs_embeds, embedding_matrix.T)\n",
    "        token_ids = torch.argmax(similarities, dim=-1)\n",
    "        return token_ids\n",
    "\n",
    "    def thoughts_forward(self, num_thoughts, thought_ids, thought_mask, num_of_thought_tokens = 1):\n",
    "        \"\"\"\n",
    "        Generate continuous thought embeddings.\n",
    "        \"\"\"\n",
    "        all_thought_outputs = []\n",
    "        batch_size = thought_ids.shape[0]\n",
    "        \n",
    "        # Get initial embeddings\n",
    "        initial_embeds = self.get_input_embeddings()(thought_ids)\n",
    "        current_embeds = initial_embeds\n",
    "        current_mask = thought_mask\n",
    "\n",
    "        for t in range(num_thoughts):\n",
    "            # Forward pass through transformer\n",
    "            outputs = self.model.forward(\n",
    "                inputs_embeds=current_embeds,\n",
    "                attention_mask=current_mask,\n",
    "                past_key_values=None,\n",
    "                use_cache=False,\n",
    "                return_dict=True,\n",
    "                output_hidden_states=True,  # Get hidden states from all layers\n",
    "            )\n",
    "            \n",
    "            # Get hidden states from all layers for better representation\n",
    "            hidden_states = outputs.hidden_states\n",
    "            \n",
    "            # Combine hidden states from different layers using attention\n",
    "            layer_attention = torch.softmax(\n",
    "                torch.randn(len(hidden_states), device=hidden_states[0].device), \n",
    "                dim=0\n",
    "            )\n",
    "            weighted_states = sum(w * h for w, h in zip(layer_attention, hidden_states))\n",
    "            \n",
    "            n = num_of_thought_tokens\n",
    "            last_hidden = weighted_states[:, -n:, :]  # [batch_size, n, hidden_size]\n",
    "            \n",
    "            # Project to lower dimension for thought space\n",
    "            thought_proj = nn.Sequential(\n",
    "                nn.Linear(last_hidden.shape[-1], self.config.hidden_size // 2),\n",
    "                nn.LayerNorm(self.config.hidden_size // 2),\n",
    "                nn.GELU()\n",
    "            ).to(last_hidden.device)\n",
    "            projected_thought = thought_proj(last_hidden)  # [batch_size, n, hidden_size // 2]\n",
    "            \n",
    "            # Add noise to increase diversity\n",
    "            noise = torch.randn_like(projected_thought) * 0.1  # Adjust noise scale as needed\n",
    "            projected_thought = projected_thought + noise\n",
    "            \n",
    "            # Project back to embedding space\n",
    "            embed_proj = nn.Linear(\n",
    "                self.config.hidden_size // 2,\n",
    "                self.config.hidden_size,\n",
    "                device=projected_thought.device\n",
    "            )\n",
    "            next_token_embeds = embed_proj(projected_thought)  # [batch_size, n, hidden_size]\n",
    "            \n",
    "            # Apply layer normalization for stability\n",
    "            next_token_embeds = nn.LayerNorm(\n",
    "                self.config.hidden_size,\n",
    "                device=next_token_embeds.device\n",
    "            )(next_token_embeds)\n",
    "            \n",
    "            # Update embeddings and mask\n",
    "            current_embeds = torch.cat([current_embeds, next_token_embeds], dim=1)\n",
    "            current_mask = torch.cat([\n",
    "                current_mask,\n",
    "                torch.ones((batch_size, n), device=current_mask.device)\n",
    "            ], dim=1)\n",
    "            \n",
    "            all_thought_outputs.append(last_hidden)\n",
    "\n",
    "        # Ensure reasonable sequence length\n",
    "        max_seq_len = self.model_config.get(\"max_length\", 512)\n",
    "        if current_embeds.shape[1] > max_seq_len:\n",
    "            current_embeds = current_embeds[:, :max_seq_len, :]\n",
    "            current_mask = current_mask[:, :max_seq_len]\n",
    "        \n",
    "        return all_thought_outputs, current_embeds, current_mask\n",
    "\n",
    "\n",
    "    def train_forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        num_logits_to_keep: int = 0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Training forward pass with continuous thought generation and CoT alignment.\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "\n",
    "        # Keep original labels if none provided\n",
    "        if labels is None:\n",
    "            labels = input_ids.clone()\n",
    "            batch_size = labels.shape[0]\n",
    "            eot_id = self.tokenizer.convert_tokens_to_ids(self.model_config[\"eot_id\"])\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                # Find the positions of <eot> in the input_ids\n",
    "                eot_pos = (input_ids[i] == eot_id).nonzero(as_tuple=True)\n",
    "\n",
    "                if len(eot_pos[0]) > 0:\n",
    "                    # Get the last occurrence of <eot>\n",
    "                    last_eot_pos = eot_pos[0][-1].item()\n",
    "                    \n",
    "                    # Mask everything before and including the last <eot>\n",
    "                    labels[i, :last_eot_pos] = -100\n",
    "\n",
    "                # Mask padding\n",
    "                labels[i, attention_mask[i] == 0] = -100\n",
    "\n",
    "        # Get input embeddings if not provided\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.get_input_embeddings()(input_ids)\n",
    "\n",
    "\n",
    "        # Generate continuous thoughts\n",
    "        if self.current_stage > 0:\n",
    "            num_thoughts = self.current_stage * self.model_config[\"continuous_thoughts\"]\n",
    "            all_thoughts, final_embeds, final_mask = self.thoughts_forward(\n",
    "                num_thoughts=num_thoughts,\n",
    "                thought_ids=input_ids,\n",
    "                thought_mask=attention_mask,\n",
    "                num_of_thought_tokens = self.model_config[\"training_thoughts_sequence_length\"]\n",
    "            )\n",
    "\n",
    "            # Add auxiliary losses\n",
    "            auxiliary_losses = []\n",
    "\n",
    "            # Thought coherence loss\n",
    "            if len(all_thoughts) > 1:\n",
    "                coherence_loss = 0\n",
    "                for t1, t2 in zip(all_thoughts[:-1], all_thoughts[1:]):\n",
    "                    sim = F.cosine_similarity(t1, t2, dim=-1)\n",
    "                    coherence_loss += (1 - sim).mean()\n",
    "                auxiliary_losses.append(coherence_loss * self.coherence_weight)\n",
    "\n",
    "            batch_size = labels.shape[0]\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                # Find the start and end of CoT in the labels\n",
    "                cot_start = None\n",
    "                \n",
    "                for j, token_id in enumerate(labels[i]):\n",
    "                    if token_id == self.tokenizer.convert_tokens_to_ids(self.model_config[\"eot_id\"]):\n",
    "                        cot_start = j + 1  # Start of CoT\n",
    "\n",
    "\n",
    "                # Debugging: Print CoT tokens and latent thoughts\n",
    "                if cot_start is not None:\n",
    "                    # Extract CoT tokens\n",
    "                    cot_tokens = labels[i, cot_start:]  # [cot_seq_len]\n",
    "\n",
    "                    # Get the latent thoughts for this batch\n",
    "                    latent_thoughts = all_thoughts[i]  # [thought_seq_len, hidden_size]\n",
    "\n",
    "                    # Project latent thoughts to logits\n",
    "                    thought_logits = self.lm_head(latent_thoughts)  # [thought_seq_len, vocab_size]\n",
    "                    thought_token_ids = torch.argmax(thought_logits, dim=-1)  # [thought_seq_len]\n",
    "\n",
    "\n",
    "                    # Debugging: Print CoT tokens and latent thoughts\n",
    "                    if self.debug:\n",
    "                        # Decode CoT tokens\n",
    "                        cot_tokens_list = cot_tokens.squeeze().tolist()  # Convert to 1D list\n",
    "                        if isinstance(cot_tokens_list, int):  # Handle single token case\n",
    "                            cot_tokens_list = [cot_tokens_list]\n",
    "                        cot_text = self.tokenizer.decode(cot_tokens_list, skip_special_tokens=True)\n",
    "                        print(f\" ==================== \\n Debug: CoT for batch {i}: {cot_text} \\n ====================\")\n",
    "\n",
    "                        # Decode latent thoughts\n",
    "                        thought_token_ids_list = thought_token_ids.squeeze().tolist()  # Convert to list\n",
    "\n",
    "                        # Ensure thought_token_ids_list is a flat list\n",
    "                        if isinstance(thought_token_ids_list, list) and all(isinstance(item, list) for item in thought_token_ids_list):\n",
    "                            # Flatten the nested list\n",
    "                            thought_token_ids_list = [token for sublist in thought_token_ids_list for token in sublist]\n",
    "                        elif isinstance(thought_token_ids_list, int):  # Handle single token case\n",
    "                            thought_token_ids_list = [thought_token_ids_list]\n",
    "\n",
    "                        # Decode the flat list of token IDs\n",
    "                        thought_text = self.tokenizer.decode(thought_token_ids_list, skip_special_tokens=False)\n",
    "                        print(f\"==================== \\n Debug: Latent thoughts for batch {i}: {thought_text} \\n ========================\")\n",
    "\n",
    "\n",
    "            # Forward pass with thoughts\n",
    "            outputs = super().forward(\n",
    "                inputs_embeds=final_embeds,\n",
    "                attention_mask=final_mask,\n",
    "                labels=labels,\n",
    "                **kwargs\n",
    "            )\n",
    "\n",
    "            # Add auxiliary losses\n",
    "            if auxiliary_losses:\n",
    "                outputs.loss += sum(auxiliary_losses)\n",
    "\n",
    "        else:\n",
    "\n",
    "            if inputs_embeds is None:\n",
    "                # Standard forward pass for initial stage\n",
    "                outputs = super().forward(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    position_ids=position_ids,\n",
    "                    inputs_embeds=inputs_embeds,\n",
    "                    labels=labels,\n",
    "                    output_attentions=output_attentions,\n",
    "                    output_hidden_states=output_hidden_states,\n",
    "                    return_dict=return_dict,\n",
    "                    **kwargs,\n",
    "                )\n",
    "            else:\n",
    "\n",
    "                outputs = super().forward(\n",
    "                    attention_mask=attention_mask,\n",
    "                    position_ids=position_ids,\n",
    "                    inputs_embeds=inputs_embeds,\n",
    "                    labels=labels,\n",
    "                    output_attentions=output_attentions,\n",
    "                    output_hidden_states=output_hidden_states,\n",
    "                    return_dict=return_dict,\n",
    "                    **kwargs,\n",
    "                )\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    \n",
    "    def infer_forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Union[DynamicCache, List[torch.FloatTensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        num_logits_to_keep: int = 0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inference forward pass with continuous thought generation.\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = input_ids.shape[0]\n",
    "\n",
    "        # Insert <bot> token to initiate latent reasoning\n",
    "        if input_ids.shape[1] > 1:\n",
    "            input_ids = torch.cat(\n",
    "                [\n",
    "                    input_ids,\n",
    "                    torch.tensor(\n",
    "                        [[self.tokenizer.convert_tokens_to_ids(self.model_config[\"bot_id\"])]] * batch_size,\n",
    "                        device=input_ids.device,\n",
    "                    ),\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "            attention_mask = torch.cat(\n",
    "                [\n",
    "                    attention_mask,\n",
    "                    torch.ones((batch_size, 1), device=attention_mask.device),\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "        # Generate continuous thoughts\n",
    "        if self.model_config[\"stages\"] - 1 > 0 and input_ids.shape[1] > 1:\n",
    "            num_thoughts = (self.model_config[\"stages\"] - 1) * self.model_config[\"continuous_thoughts\"]\n",
    "            all_thoughts, final_embeds, final_mask = self.thoughts_forward(\n",
    "                num_thoughts, input_ids, attention_mask\n",
    "            )\n",
    "\n",
    "            # Add <eot> token to mark the end of latent reasoning\n",
    "            eot_embeds = self.get_input_embeddings()(\n",
    "                torch.tensor(\n",
    "                    [[self.tokenizer.convert_tokens_to_ids(self.model_config[\"eot_id\"])]] * batch_size,\n",
    "                    device=final_embeds.device,\n",
    "                )\n",
    "            )\n",
    "            final_embeds = torch.cat([final_embeds, eot_embeds], dim=1)\n",
    "            final_mask = torch.cat([final_mask, torch.ones((batch_size, 1), device=final_mask.device)], dim=1)\n",
    "\n",
    "            # Generate final output in language mode\n",
    "            outputs = super().forward(\n",
    "                inputs_embeds=final_embeds,\n",
    "                attention_mask=final_mask,\n",
    "                past_key_values=None,  # Reset past_key_values for answer generation\n",
    "                use_cache=use_cache,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "                **kwargs,\n",
    "            )\n",
    "        else:\n",
    "            # Standard forward pass (no latent thoughts)\n",
    "            outputs = super().forward(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                position_ids=position_ids,\n",
    "                past_key_values=past_key_values,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                labels=labels,\n",
    "                use_cache=use_cache,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Union[DynamicCache, List[torch.FloatTensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        num_logits_to_keep: int = 0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Main forward function that routes to either training or inference.\"\"\"\n",
    "        forward_fn = self.train_forward if self.training else self.infer_forward\n",
    "        return forward_fn(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            labels=labels,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "            num_logits_to_keep=num_logits_to_keep,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type gemma2 to instantiate a model of type gemma. This is not supported for all configurations of models and can yield errors.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config[\"model_name\"])\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Add special tokens\n",
    "special_tokens = {\n",
    "    \"additional_special_tokens\": [config[\"bot_id\"], config[\"eot_id\"], config[\"answer_id\"]]\n",
    "}\n",
    "num_added_tokens = tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "# Load the Reasoning model configuration\n",
    "model_config = AutoConfig.from_pretrained(config[\"model_name\"])\n",
    "latent_config = LatentReasoningGemmaForCausalLM.DEFAULT_CONFIG\n",
    "LatentReasoningGemmaForCausalLM.DEFAULT_CONFIG = {\n",
    "    **latent_config,\n",
    "    **config\n",
    "}\n",
    "updated_latent_config = LatentReasoningGemmaForCausalLM.DEFAULT_CONFIG\n",
    "model = LatentReasoningGemmaForCausalLM(config=model_config)\n",
    "\n",
    "# Load the Reasoning model\n",
    "model = model.from_pretrained(\n",
    "    config[\"model_name\"],\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "model.tokenizer = tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "# Load the normal model for comparison\n",
    "model_without_reasoning = AutoModelForCausalLM.from_pretrained(config[\"model_name\"])\n",
    "model_without_reasoning.resize_token_embeddings(len(tokenizer))\n",
    "model_without_reasoning = model_without_reasoning.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create Helper functions for inferencing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencing Helper Function\n",
    "\n",
    "- Chain-of-Thought (CoT) Decoding: A technique where the model generates intermediate reasoning steps before producing the final answer.\n",
    "\n",
    "- Top-K Sampling: Selecting the k most likely tokens to explore multiple possible continuations.\n",
    "\n",
    "- Temperature: A parameter that controls the randomness of predictions. Higher values make the output more diverse, while lower values make it more deterministic.\n",
    "\n",
    "- Min-Margin Confidence: A measure of how confident the model is in its predictions, based on the difference between the best and second-best probabilities.\n",
    "\n",
    "\n",
    "Example Workflow:\n",
    "\n",
    "You ask a question: `What is the capital of France?`\n",
    "\n",
    "generate_answer explores multiple possible answers `(e.g., \"Paris\", \"London\", \"Berlin\")`.\n",
    "\n",
    "It calculates the confidence for each answer and selects the one with the highest confidence `(e.g., \"Paris\")`.\n",
    "\n",
    "The final answer \"Paris\" is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from transformers import TextStreamer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def generate_answer(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    question: str,\n",
    "    max_length: int = 128,\n",
    "    k: int = config[\"cot_decoding_k\"],\n",
    "    temperature: float = 1.0,\n",
    "    **generation_kwargs\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generates answer using CoT decoding and returns the best path.\n",
    "    \n",
    "    Args:\n",
    "        model: The language model\n",
    "        tokenizer: The tokenizer\n",
    "        question: Input question\n",
    "        max_length: Maximum sequence length\n",
    "        k: Number of alternative paths to consider\n",
    "        temperature: Sampling temperature\n",
    "        **generation_kwargs: Additional generation arguments\n",
    "        \n",
    "    Returns:\n",
    "        Best decoded sequence with highest confidence\n",
    "    \"\"\"\n",
    "    # Initialize streamer\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=False, skip_special_tokens=False)\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(question, max_length=max_length, return_tensors=\"pt\").to(model.device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    # Get initial logits for CoT paths\n",
    "    outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        output_hidden_states=True,\n",
    "        return_dict=True\n",
    "    )\n",
    "    \n",
    "    first_token_logits = outputs.logits[:, -1, :] / temperature\n",
    "    \n",
    "    # Get top-k tokens\n",
    "    probs = F.softmax(first_token_logits, dim=-1)\n",
    "    top_k_probs, top_k_tokens = torch.topk(probs, k, dim=-1)\n",
    "    \n",
    "    best_path = None\n",
    "    best_confidence = -float('inf')\n",
    "    \n",
    "    # Generate continuation for each top-k token\n",
    "    for i in range(k):\n",
    "        # Prepare input with current top-k token\n",
    "        curr_input_ids = torch.cat([\n",
    "            input_ids,\n",
    "            top_k_tokens[:, i:i+1]\n",
    "        ], dim=1)\n",
    "        \n",
    "        curr_attention_mask = torch.cat([\n",
    "            attention_mask,\n",
    "            torch.ones((attention_mask.shape[0], 1), device=model.device)\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Generate with streamer for best path\n",
    "        outputs = model.generate(\n",
    "            input_ids=curr_input_ids,\n",
    "            attention_mask=curr_attention_mask,\n",
    "            max_length=max_length,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            output_scores=True,\n",
    "            return_dict_in_generate=True,\n",
    "            streamer=streamer if i == 0 else None,  # Only stream first path\n",
    "            **generation_kwargs\n",
    "        )\n",
    "        \n",
    "        # Calculate confidence for this path\n",
    "        _, confidence = calculate_answer_confidence(\n",
    "            outputs.sequences[0].tolist(),\n",
    "            outputs.scores[-1],\n",
    "            tokenizer\n",
    "        )\n",
    "        \n",
    "        # Update best path if confidence is higher\n",
    "        if confidence > best_confidence:\n",
    "            best_confidence = confidence\n",
    "            best_path = outputs.sequences[0]\n",
    "            \n",
    "    # Return the path with highest confidence\n",
    "    return tokenizer.decode(best_path, skip_special_tokens=True)\n",
    "\n",
    "def calculate_answer_confidence(\n",
    "    sequence: List[int],\n",
    "    final_logits: torch.Tensor,\n",
    "    tokenizer: PreTrainedTokenizer\n",
    ") -> Tuple[str, float]:\n",
    "    \"\"\"Calculate confidence score using min-margin approach.\"\"\"\n",
    "    # Extract answer from sequence\n",
    "    answer = extract_answer(sequence, tokenizer)\n",
    "    \n",
    "    if not answer:\n",
    "        return \"\", 0.0\n",
    "    \n",
    "    # Get probabilities\n",
    "    probs = F.softmax(final_logits, dim=-1)\n",
    "    \n",
    "    # Calculate margins for answer tokens\n",
    "    answer_tokens = tokenizer.encode(answer, add_special_tokens=False)\n",
    "    margins = []\n",
    "    \n",
    "    for token in answer_tokens:\n",
    "        token_prob = probs[0, token].item()\n",
    "        sorted_probs, _ = torch.sort(probs, dim=-1, descending=True)\n",
    "        second_best_prob = sorted_probs[0, 1].item()\n",
    "        margin = token_prob - second_best_prob\n",
    "        margins.append(margin)\n",
    "        \n",
    "    confidence = sum(margins) / len(margins)\n",
    "    return answer, confidence\n",
    "\n",
    "def extract_answer(sequence: List[int], tokenizer: PreTrainedTokenizer) -> str:\n",
    "    \"\"\"\n",
    "    Extract final answer from sequence using <eot> token.\n",
    "    Finds the answer between the last occurrence of <eot> and the end of sequence.\n",
    "    \"\"\"\n",
    "    # Convert sequence to string\n",
    "    decoded = tokenizer.decode(sequence)\n",
    "    \n",
    "    # Find last <eot> position\n",
    "    eot_position = decoded.rfind(config[\"eot_id\"])\n",
    "    \n",
    "    if eot_position != -1:\n",
    "        # Extract everything after the last <eot>\n",
    "        answer = decoded[eot_position + len(config[\"eot_id\"]):].strip()\n",
    "        return answer\n",
    "        \n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "tick_start = 0\n",
    "\n",
    "def tick():\n",
    "    global tick_start\n",
    "    tick_start = time.time()\n",
    "\n",
    "def tock():\n",
    "    print(f\"TOTAL TIME ELAPSED: {time.time() - tick_start:.2f}s\")\n",
    "\n",
    "\n",
    "def text_gen(prompt, model, tokenizer):\n",
    "    tick()\n",
    "    input = f\"{prompt}\"\n",
    "    print(f\"Question: {prompt} \\n ==========================================\")\n",
    "    output = generate_answer(model=model, tokenizer=tokenizer, question=input, k=5, max_length=config[\"max_length\"] )\n",
    "    print(f\"Outputs: ========================\")\n",
    "    print(output)\n",
    "    tock()\n",
    "    print(f\"\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the capabilities of normal model before fine tuning the reasoning model.\n",
    "\n",
    "From the result, you'd notice it is not very good. It struggles with switching translation,  verbose is quite long and it takes a long time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "The 'batch_size' argument of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'max_batch_size' argument instead.\n",
      "The 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 格闘家ボブ・サップの出身国はどこでしょう？ \n",
      " ==========================================\n",
      "<bos>格闘家ボブ・サップの出身国はどこでしょう？\n",
      "\n",
      "ボブ・サップは、アメリカ合衆国出身の格闘家です。\n",
      "\n",
      "ボブ・サップは、アメリカ合衆国出身の格闘家です。\n",
      "\n",
      "ボブ・サップは、アメリカ合衆国出身の格闘家です。\n",
      "\n",
      "ボブ・サップは、アメリカ合衆国出身の格闘家です。\n",
      "\n",
      "ボブ・サップは、アメリカ合衆国出身の格闘家です。\n",
      "\n",
      "ボブ・サップは、アメリカ合衆国出身の格闘家です。\n",
      "\n",
      "ボブ・サップは、アメリカ合衆国出身の格闘家です。\n",
      "\n",
      "ボブ・サップは、アメリカ合衆国出身の格闘家です。\n",
      "\n",
      "ボブ・サップは、アメリカ合衆国出身の格闘家です。\n",
      "\n",
      "ボブ・サップは、アメリカ合衆国出身の格闘家です。\n",
      "\n",
      "ボブ・サップは、アメリカ合衆国出身の格闘家です。\n",
      "\n",
      "ボブ・サップは、アメリカ合衆国出身の格闘家です。\n",
      "\n",
      "ボブ・サップは、アメリカ合衆国出身の格闘家です。\n",
      "\n",
      "ボブ・サップは\n",
      "Outputs: ========================\n",
      "格闘家ボブ・サップの出身国はどこでしょう？ボブ・サップはアメリカ人です。ボブ・サップはアメリカ人です。ボブ・サップはアメリカ人です。ボブ・サップはアメリカ人です。ボブ・サップはアメリカ人です。ボブ・サップはアメリカ人です。ボブ・サップはアメリカ人です。ボブ・サップはアメリカ人です。ボブ・サップはアメリカ人です。ボブ・サップはアメリカ人です。ボブ・サップはアメリカ人です。ボブ・サップはアメリカ人です。ボブ・サップはアメリカ人です。ボブ・サップはアメリカ人です。ボブ・サップはアメリカ人です。ボブ・サップはアメリカ人です。ボブ・サップはアメリカ人です。ボブ・サップはアメリカ人です。ボブ・サップはアメリカ人です。ボブ・サップはアメリカ人です。ボブ・サップはアメリカ人です。ボブ・サップはアメリカ人です。ボブ・サップはアメリカ人です。ボブ・サップはアメリカ人です。ボ\n",
      "TOTAL TIME ELAPSED: 34.45s\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 人気漫画『ドラえもん』の登場人物で、ジャイアンの苗字は剛田ですが、スネ夫の苗字は何でしょう？ \n",
      " ==========================================\n",
      "<bos>人気漫画『ドラえもん』の登場人物で、ジャイアンの苗字は剛田ですが、スネ夫の苗字は何でしょう？\n",
      "\n",
      "スネ夫の苗字は、<strong>「剛田」</strong>です。\n",
      "\n",
      "ジャイアンの苗字は「剛田」ですが、スネ夫の苗字は「剛田」です。\n",
      "\n",
      "ジャイアンの苗字は「剛田」ですが、スネ夫の苗字は「剛田」です。\n",
      "\n",
      "ジャイアンの苗字は「剛田」ですが、スネ夫の苗字は「剛田」です。\n",
      "\n",
      "ジャイアンの苗字は「剛田」ですが、スネ夫の苗字は「剛田」です。\n",
      "\n",
      "ジャイアンの苗字は「剛田」ですが、スネ夫の苗字は「剛田」です。\n",
      "\n",
      "ジャイアンの苗字は「剛田」ですが、スネ夫の苗字は「剛田」です。\n",
      "\n",
      "ジャイアンの苗字は「剛田」ですが、スネ夫の苗字は「剛田」です。\n",
      "\n",
      "ジャイアンの苗字は「剛田」ですが、スネ夫の苗\n",
      "Outputs: ========================\n",
      "人気漫画『ドラえもん』の登場人物で、ジャイアンの苗字は剛田ですが、スネ夫の苗字は何でしょう？ドラえもんの登場人物は、ジャイアン、スネ夫、ひみつ道具、ドラえもん、ドラえもんのひみつ道具、ドラえもんのひみつ道具のひみつ道具、ドラえもんのひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具のひみつ道具\n",
      "TOTAL TIME ELAPSED: 31.10s\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: Translate 'Hello, how are you?' to Japanese. \n",
      " ==========================================\n",
      "<bos>Translate 'Hello, how are you?' to Japanese.\n",
      "\n",
      "Translate 'I'm fine, thank you' to Japanese.\n",
      "\n",
      "Translate 'I'm fine, thank you' to Japanese.\n",
      "\n",
      "Translate 'I'm fine, thank you' to Japanese.\n",
      "\n",
      "Translate 'I'm fine, thank you' to Japanese.\n",
      "\n",
      "Translate 'I'm fine, thank you' to Japanese.\n",
      "\n",
      "Translate 'I'm fine, thank you' to Japanese.\n",
      "\n",
      "Translate 'I'm fine, thank you' to Japanese.\n",
      "\n",
      "Translate 'I'm fine, thank you' to Japanese.\n",
      "\n",
      "Translate 'I'm fine, thank you' to Japanese.\n",
      "\n",
      "Translate 'I'm fine, thank you' to Japanese.\n",
      "\n",
      "Translate 'I'm fine, thank you' to Japanese.\n",
      "\n",
      "Translate 'I'm fine, thank you' to Japanese.\n",
      "\n",
      "Translate 'I'm fine, thank you' to Japanese.\n",
      "\n",
      "Translate 'I'm fine, thank you' to Japanese.\n",
      "\n",
      "Translate 'I'm fine, thank you' to Japanese.\n",
      "\n",
      "Translate 'I'm fine, thank you' to Japanese.\n",
      "\n",
      "Translate 'I'm fine, thank you' to Japanese.\n",
      "\n",
      "Translate 'I'm\n",
      "Outputs: ========================\n",
      "Translate 'Hello, how are you?' to Japanese. Hello, how are you?\n",
      "\n",
      "[Answer 1]\n",
      "\n",
      "    こんにちは、どうお過ごしですか？\n",
      "\n",
      "    Konnichiwa, dou osusugidesuka?\n",
      "\n",
      "    Hello, how are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you doing?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    How are you?\n",
      "\n",
      "    \n",
      "TOTAL TIME ELAPSED: 27.76s\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: 「お元気ですか」を英語に訳すと \n",
      " ==========================================\n",
      "<bos>「お元気ですか」を英語に訳すと「Are you okay?」となります。\n",
      "\n",
      "「元気ですか」は「Are you okay?」と訳すことができます。\n",
      "\n",
      "「元気ですか」は「Are you okay?」と訳すことができます。\n",
      "\n",
      "「元気ですか」は「Are you okay?」と訳すことができます。\n",
      "\n",
      "「元気ですか」は「Are you okay?」と訳すことができます。\n",
      "\n",
      "「元気ですか」は「Are you okay?」と訳すことができます。\n",
      "\n",
      "「元気ですか」は「Are you okay?」と訳すことができます。\n",
      "\n",
      "「元気ですか」は「Are you okay?」と訳すことができます。\n",
      "\n",
      "「元気ですか」は「Are you okay?」と訳すことができます。\n",
      "\n",
      "「元気ですか」は「Are you okay?」と訳すことができます。\n",
      "\n",
      "「元気ですか」は「Are you okay?」と訳すことができます。\n",
      "\n",
      "「元気ですか」は「Are you okay?」と訳すことができます。\n",
      "\n",
      "「元気ですか」は「Are you okay?」と訳すことができます。\n",
      "\n",
      "「元気ですか」は「Are you okay?」と訳すことができます。\n",
      "\n",
      "「元気ですか」は「Are you okay?」と訳すことができます。\n",
      "\n",
      "「元気ですか」は「Are you okay?」と訳す\n",
      "Outputs: ========================\n",
      "「お元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「元気ですか」を英語に訳すと？\n",
      "\n",
      "「\n",
      "TOTAL TIME ELAPSED: 34.28s\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question: Translate to english `「ねえ、それは何のためにあるの？` \n",
      " ==========================================\n",
      "<bos>Translate to english `「ねえ、それは何のためにあるの？`\n",
      "\n",
      "[User 0001]\n",
      "\n",
      "Hello,\n",
      "\n",
      "I'm trying to translate this sentence:\n",
      "\n",
      "「ねえ、それは何のためにあるの？\n",
      "\n",
      "I'm not sure if it's a question or a statement.\n",
      "\n",
      "I'm thinking it's a question, but I'm not sure.\n",
      "\n",
      "Thanks!\n",
      " \n",
      "\n",
      "[User 0002]\n",
      "\n",
      "It's a question.\n",
      " \n",
      "\n",
      "[User 0003]\n",
      "\n",
      "It's a question.\n",
      " \n",
      "\n",
      "[User 0004]\n",
      "\n",
      "It's a question.\n",
      " \n",
      "\n",
      "[User 0005]\n",
      "\n",
      "It's a question.\n",
      " \n",
      "\n",
      "[User 0006]\n",
      "\n",
      "It's a question.\n",
      " \n",
      "\n",
      "[User 0007]\n",
      "\n",
      "It's a question.\n",
      " \n",
      "\n",
      "[User 0008]\n",
      "\n",
      "It's a question.\n",
      " \n",
      "\n",
      "[User 0009]\n",
      "\n",
      "It's a question.\n",
      " \n",
      "\n",
      "[User 0010]\n",
      "\n",
      "It's a question.\n",
      " \n",
      "\n",
      "[User 0011\n",
      "Outputs: ========================\n",
      "Translate to english `「ねえ、それは何のためにあるの？` 「それは、私のために作られたものだ」 「それは、私のために作られたものだ」 「それは、私のために作られたものだ」 「それは、私のために作られたものだ」 「それは、私のために作られたものだ」 「それは、私のために作られたものだ」 「それは、私のために作られたものだ」 「それは、私のために作られたものだ」 「それは、私のために作られたものだ」 「それは、私のために作られたものだ」 「それは、私のために作られたものだ」 「それは、私のために作られたものだ」 「それは、私のために作られたものだ」 「それは、私のために作られたものだ」 「それは、私のために作られたものだ」 「それは、私のために作られたものだ」 「それは、私のために作られたものだ」 「それは、私のために作られたものだ」 「それは、私のために作られたものだ」 「それは、私のために作られたものだ」 「それは、私のために作られたものだ」 「それは、私のために作られたものだ」 「それは、私のために作られたものだ」 「それは、私のために作られたものだ」 「\n",
      "TOTAL TIME ELAPSED: 33.70s\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the function\n",
    "text_gen(\"格闘家ボブ・サップの出身国はどこでしょう？\", model=model_without_reasoning, tokenizer=tokenizer)\n",
    "text_gen(\"人気漫画『ドラえもん』の登場人物で、ジャイアンの苗字は剛田ですが、スネ夫の苗字は何でしょう？\",  model=model_without_reasoning, tokenizer=tokenizer)\n",
    "# text_gen(\"Translate 'Hello, how are you?' to Japanese.\",  model=model_without_reasoning, tokenizer=tokenizer)\n",
    "# text_gen(\"「お元気ですか」を英語に訳すと\",  model=model_without_reasoning, tokenizer=tokenizer)\n",
    "# text_gen(\"Translate to english `「ねえ、それは何のためにあるの？`\", model=model_without_reasoning, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7) Training\n",
    "\n",
    "The training implementation follows a multi-stage curriculum based on Hao et al. (2024), gradually introducing continuous latent reasoning. Each stage represents a step in transitioning from pure language processing to latent space reasoning:\n",
    "\n",
    "\n",
    "Using Hugging Face's Trainer with optimizations:\n",
    "- BFloat16 precision\n",
    "- 8-bit Adam optimizer \n",
    "- Gradient accumulation\n",
    "- WandB tracking\n",
    "- Checkpoint management\n",
    "\n",
    "The model progressively learns to leverage continuous thought states while preserving translation capabilities, with each stage building upon the previous one's learned representations.\n",
    "\n",
    "To train, you need to get ready your wandb token Id as we report the logs to wandb.\n",
    "To get your wandb key, visit [Wandb](https://wandb.ai/quickstart?utm_source=app-resource-center&utm_medium=app&utm_term=quickstart)\n",
    "\n",
    "Let's see this in action..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwassname\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/wassname/SGIronWolf/projects5/2025/COCONUT/latent-gemma/wandb/run-20250111_073838-bixe7zsd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wassname/gemma2-finetuning/runs/bixe7zsd' target=\"_blank\">mild-plant-3</a></strong> to <a href='https://wandb.ai/wassname/gemma2-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wassname/gemma2-finetuning' target=\"_blank\">https://wandb.ai/wassname/gemma2-finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wassname/gemma2-finetuning/runs/bixe7zsd' target=\"_blank\">https://wandb.ai/wassname/gemma2-finetuning/runs/bixe7zsd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/2025/COCONUT/latent-gemma/.venv/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f70d691cf6c94c34888713f29a9c8507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ") \n",
    "import wandb\n",
    "import os\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Initialize WandB\n",
    "wandb.init(project=config[\"wandb_project\"], config=config)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config[\"output_dir\"],\n",
    "    per_device_train_batch_size=config[\"per_device_train_batch_size\"],\n",
    "    gradient_accumulation_steps=config[\"gradient_accumulation_steps\"],\n",
    "    learning_rate=config[\"learning_rate\"],\n",
    "    warmup_ratio=config[\"warmup_steps\"],\n",
    "    logging_steps=config[\"logging_steps\"],\n",
    "    save_steps=config[\"save_steps\"],\n",
    "    bf16=config[\"bf16\"],\n",
    "    bf16_full_eval=config[\"bf16_full_eval\"],\n",
    "    optim=config[\"optim\"],\n",
    "    report_to=\"wandb\",\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=True,\n",
    "    # gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "# Move model to GPU and wrap with DataParallel if multiple GPUs available\n",
    "if torch.cuda.is_available():\n",
    "    # Check if model is not already on CUDA\n",
    "    if not next(model.parameters()).is_cuda:\n",
    "        model = model.cuda()\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        # Check if model isn't already wrapped with DataParallel\n",
    "        if not isinstance(model, torch.nn.DataParallel):\n",
    "            # Use DataParallel with explicit device IDs\n",
    "            model = torch.nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "\n",
    "def stage_trainer(stage=0):\n",
    "\n",
    "    if isinstance(model, torch.nn.DataParallel):\n",
    "        model.module.current_stage = stage\n",
    "    else:\n",
    "        model.current_stage = stage\n",
    "\n",
    "    current_output_dir = f\"{config['output_dir']}_stage{stage}\"\n",
    "    training_args.output_dir = current_output_dir\n",
    "    training_args.num_train_epochs = 3\n",
    "        \n",
    "\n",
    "    # Load the Reasoning model configuration\n",
    "    dataset_ = dataset.map(\n",
    "        (lambda x: preprocess_function(\n",
    "            x, \n",
    "            detector=detector,\n",
    "            stages=stage, \n",
    "            eos_token=tokenizer.eos_token,\n",
    "            bos_token=tokenizer.bos_token,\n",
    "            language_config=language_config\n",
    "        )),\n",
    "        batched=True,\n",
    "        batch_size=config[\"batch_size\"]\n",
    "    )\n",
    "\n",
    "    # Tokenize the dataset\n",
    "    dataset_ = dataset_.map(\n",
    "        (lambda x: tokenizer_function(\n",
    "            x, \n",
    "            tokenizer=tokenizer,\n",
    "        )),\n",
    "        batched=True,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        remove_columns=[\"input\", \"instruction\", \"output\", \"prompt\"]\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset_[\"train\"]\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "\n",
    "    # Save checkpoints\n",
    "    for folder in os.listdir(current_output_dir):\n",
    "        if folder.startswith(\"checkpoint-\"):\n",
    "            checkpoint_folder = os.path.join(current_output_dir, folder)\n",
    "            if os.path.isdir(checkpoint_folder):\n",
    "                tokenizer.save_pretrained(checkpoint_folder)\n",
    "                # If using DataParallel, save the base model\n",
    "                model_to_save = model.module if hasattr(model, 'module') else model\n",
    "                model_to_save.save_pretrained(checkpoint_folder)\n",
    "\n",
    "# Run training stages\n",
    "for stage in range(config[\"stages\"] + 1):\n",
    "    stage_trainer(stage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we done training, let's load our fine tuned model for inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def load_model(model_name = \"output_stage1/checkpoint-10000\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    model_config = AutoConfig.from_pretrained(model_name)\n",
    "    model = LatentReasoningGemmaForCausalLM(config=model_config)\n",
    "    model = model.from_pretrained(model_name)\n",
    "    model.tokenizer = tokenizer\n",
    "\n",
    "    model = model.cuda()\n",
    "\n",
    "    return  model, tokenizer\n",
    "\n",
    "\n",
    "# Make sure to load the model from your specified path. In our case our path is \"output_stage1/checkpoint-10000\"\n",
    "model, tokenizer = load_model(model_name= \"output_stage1/checkpoint-10000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test after fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "text_gen(\"格闘家ボブ・サップの出身国はどこでしょう？\", model=model, tokenizer=tokenizer)\n",
    "text_gen(\"人気漫画『ドラえもん』の登場人物で、ジャイアンの苗字は剛田ですが、スネ夫の苗字は何でしょう？\", model=model, tokenizer=tokenizer)\n",
    "text_gen(\"「お元気ですか」を英語に訳すと \", model=model, tokenizer=tokenizer)\n",
    "text_gen(\"Translate to english `「ねえ、それは何のためにあるの？`\", model=model, tokenizer=tokenizer)\n",
    "text_gen(\"「abc ～the first～」へようこそ！さて、ABC・・・と始まるアルファベットは、全部で何文字でしょう？`\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8) Evaulation\n",
    "\n",
    "Our evaluation framework employs multiple metrics to provide a thorough assessment of model performance, going beyond simple exact matching to capture various aspects of answer quality. But before we do, we need to preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.download('punkt_tab')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def preprocess_eval_dataset_function(\n",
    "    examples, \n",
    "):\n",
    "    \"\"\"\n",
    "    Preprocess the input examples by constructing the prompt with reasoning steps.\n",
    "\n",
    "    Args:\n",
    "        examples (dict): A dictionary containing the input examples with keys \"instruction\", \"input\", and \"output\".\n",
    "    Returns:\n",
    "        dict: A dictionary containing the preprocessed prompts.\n",
    "    \"\"\"\n",
    "\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "\n",
    "    new_inputs = []\n",
    "    \n",
    "    for i in range(len(instructions)):\n",
    "        instruction = instructions[i]\n",
    "        input = inputs[i]\n",
    "\n",
    "        input = instruction + input\n",
    "        new_inputs.append(input)\n",
    "\n",
    "\n",
    "    return {\"input\": new_inputs, \"output\": outputs, \"instructions\": instructions}\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess eval dataset\n",
    "eval_dataset_ = eval_dataset.map(\n",
    "    preprocess_eval_dataset_function,\n",
    "    batched=True,\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics\n",
    "We implement several complementary metrics to evaluate model performance:\n",
    "\n",
    "- Fuzzy Matching Accuracy\n",
    "\n",
    "    - Uses the FuzzyWuzzy algorithm to compute string similarity\n",
    "    - Accounts for minor variations in text (e.g., spacing, capitalization)\n",
    "    - Considers answers correct when similarity exceeds a configured threshold\n",
    "\n",
    "\n",
    "- BLEU Score\n",
    "\n",
    "    - Evaluates the precision of n-gram matches\n",
    "    - Provides a complementary perspective to ROUGE metrics\n",
    "    - Useful for assessing translation quality aspects of the answers\n",
    "\n",
    "\n",
    "- BERTScore\n",
    "\n",
    "    - Leverages contextual embeddings to capture semantic similarity\n",
    "    - More robust to paraphrasing than n-gram based metrics\n",
    "    - Correlates well with human judgments\n",
    "\n",
    "\n",
    "<img src=\"https://res.cloudinary.com/vickie/image/upload/v1735437159/uzbhzyhmkhyegetyyrmg.png\" alt=\"https://ritikjain51.medium.com/llms-fine-tuning-and-evaluation-f019515b1c67\" width=\"400\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Dict, List, Union\n",
    "from transformers import PreTrainedTokenizer, PreTrainedModel\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from thefuzz import fuzz\n",
    "from bert_score import score as bert_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import tqdm\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "\n",
    "@dataclass\n",
    "class EvaluationMetrics:\n",
    "    accuracy: float\n",
    "    avg_fuzzy_score: float\n",
    "    avg_bleu_score: float\n",
    "    avg_bert_score_f1: float\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, float]:\n",
    "        return {\n",
    "            'accuracy': self.accuracy,\n",
    "            'avg_fuzzy_score': self.avg_fuzzy_score,\n",
    "            'avg_bleu_score': self.avg_bleu_score,\n",
    "            'avg_bert_score_f1': self.avg_bert_score_f1\n",
    "        }\n",
    "\n",
    "\n",
    "def extract_answer_from_predicted_answer(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the text after '答え：' or 'Answer:' from the input text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text containing the answer.\n",
    "    \n",
    "    Returns:\n",
    "        str: The extracted answer, or an empty string if no match is found.\n",
    "    \"\"\"\n",
    "    prefixes = [\"答え：\", \"Answer:\"]\n",
    "    \n",
    "    for prefix in prefixes:\n",
    "        if prefix in text:\n",
    "            return text.split(prefix, 1)[1].strip()\n",
    "    \n",
    "    return text.strip()  # Return stripped text if no prefix found\n",
    "\n",
    "\n",
    "\n",
    "# Detect if the text contains Japanese characters\n",
    "def contains_japanese(text):\n",
    "    # Hiragana (3040-309F), Katakana (30A0-30FF), Kanji (4E00-9FFF)\n",
    "    for char in text:\n",
    "        if ('\\u3040' <= char <= '\\u309F' or  # Hiragana\n",
    "            '\\u30A0' <= char <= '\\u30FF' or  # Katakana\n",
    "            '\\u4E00' <= char <= '\\u9FFF'):   # Kanji\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def tokenize_text(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokenize text based on language (Japanese or English).\n",
    "    For Japanese, splits on spaces and punctuation while preserving important characters.\n",
    "    For English, uses basic word tokenization.\n",
    "    \"\"\"\n",
    "\n",
    "    if contains_japanese(text):\n",
    "        # Simple Japanese tokenization: split on spaces and basic punctuation\n",
    "        # while preserving Japanese punctuation\n",
    "        import re\n",
    "        # Split on spaces and common punctuation, but preserve Japanese punctuation\n",
    "        tokens = re.findall(r'[^\\s\\.,!?]+|[。、！？]', text)\n",
    "        return [token for token in tokens if token.strip()]\n",
    "    else:\n",
    "        # For English, use simple whitespace and punctuation splitting\n",
    "        import re\n",
    "        return re.findall(r'\\w+|[^\\w\\s]', text.lower())\n",
    "\n",
    "\n",
    "def compute_metrics(pred_answer: str, target_answer: str, threshold: int = 80) -> Dict[str, Union[float, bool]]:\n",
    "    \"\"\"\n",
    "    Compute multiple evaluation metrics for comparing predicted and target answers.\n",
    "    \"\"\"\n",
    "    # Preprocess answers\n",
    "    pred_clean = extract_answer_from_predicted_answer(pred_answer)\n",
    "    target_clean = target_answer.strip()\n",
    "    \n",
    "    # Convert to lowercase for consistent comparison\n",
    "    pred_lower = pred_clean.lower()\n",
    "    target_lower = target_clean.lower()\n",
    "    \n",
    "    # Calculate fuzzy match score\n",
    "    fuzzy_score = fuzz.ratio(pred_lower, target_lower)\n",
    "    \n",
    "    # Tokenize for BLEU score\n",
    "    pred_tokens = word_tokenize(pred_lower)\n",
    "    target_tokens = word_tokenize(target_lower)\n",
    "    \n",
    "    # Calculate BLEU score\n",
    "    try:\n",
    "        bleu = sentence_bleu([target_tokens], pred_tokens, weights=(1.0,))\n",
    "    except ZeroDivisionError:\n",
    "        bleu = 0.0\n",
    "\n",
    "    \n",
    "    \n",
    "    # Set language based on content\n",
    "    lang = 'ja' if contains_japanese(target_clean) else 'en'\n",
    "    \n",
    "    # Calculate BERTScore with appropriate language model\n",
    "    P, R, F1 = bert_score([pred_clean], [target_clean], lang=lang, verbose=False)\n",
    "    bert_f1 = F1.item()\n",
    "    \n",
    "    return {\n",
    "        'fuzzy_match': fuzzy_score >= threshold,\n",
    "        'fuzzy_score': fuzzy_score,\n",
    "        'bleu_score': bleu,\n",
    "        'bert_score_f1': bert_f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our evaluation helper function. We loop through the batch, call the generate function to \n",
    "get the output from the model and then compare it with the dataset output using the fuzzy matcher.dataset\n",
    "If it is correct, we add it up to the list of correct responses, if not we do not.\n",
    "\n",
    "This is how we figure out the metrics of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    dataloader,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    model: PreTrainedModel,\n",
    "    max_new_tokens: int,\n",
    "    threshold: int = 80,\n",
    ") -> EvaluationMetrics:\n",
    "    \"\"\"\n",
    "    Evaluate the model using multiple metrics.\n",
    "    \n",
    "    Returns:\n",
    "        EvaluationMetrics: Object containing all computed metrics\n",
    "    \"\"\"\n",
    "    total_instances = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "    # Initialize metric aggregators\n",
    "    total_metrics = {\n",
    "        'fuzzy_score': 0,\n",
    "        'bleu_score': 0,\n",
    "        'bert_score_f1': 0\n",
    "    }\n",
    "\n",
    "    for batch in tqdm.tqdm(dataloader):\n",
    "        inputs = batch[\"input\"]\n",
    "        outputs = batch[\"output\"]\n",
    "        batch_size = len(inputs)\n",
    "        total_instances += batch_size\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            input_text = inputs[i]\n",
    "            target_answer = outputs[i]\n",
    "\n",
    "            # Generate the answer\n",
    "            pred_answer = generate_answer(\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                question=input_text,\n",
    "                max_length=max_new_tokens,\n",
    "            )\n",
    "\n",
    "            # Compute all metrics\n",
    "            metrics = compute_metrics(pred_answer, target_answer, threshold)\n",
    "            \n",
    "            # Update counters\n",
    "            if metrics['fuzzy_match']:\n",
    "                total_correct += 1\n",
    "            \n",
    "            # Aggregate metrics\n",
    "            for key in total_metrics:\n",
    "                total_metrics[key] += metrics[key]\n",
    "\n",
    "            if config[\"debug\"]:\n",
    "                pred_answer_extracted = extract_answer_from_predicted_answer(pred_answer)\n",
    "                print(\n",
    "                    f\"Input: {input_text}\\n\"\n",
    "                    f\"Target: {target_answer}\\n\"\n",
    "                    f\"Predicted: {pred_answer_extracted}\\n\"\n",
    "                    f\"Metrics: {metrics}\\n\"\n",
    "                )\n",
    "\n",
    "    # Calculate averages\n",
    "    accuracy = total_correct / total_instances\n",
    "    for key in total_metrics:\n",
    "        total_metrics[key] /= total_instances\n",
    "\n",
    "    return EvaluationMetrics(\n",
    "        accuracy=accuracy,\n",
    "        avg_fuzzy_score=total_metrics['fuzzy_score'],\n",
    "        avg_bleu_score=total_metrics['bleu_score'],\n",
    "        avg_bert_score_f1=total_metrics['bert_score_f1']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load data for evaluation\n",
    "dataloader = DataLoader(eval_dataset_[\"test\"], batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "def test_evaluation(model, tokenizer):\n",
    "    metrics = evaluate(dataloader, tokenizer, model, config[\"max_length\"])\n",
    "    print(f\"Metrics: {metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating every model stage\n",
    "\n",
    "for i in range(config[\"stages\"] + 1):\n",
    "    model_name = f\"output_stage{i}/checkpoint-10000\"\n",
    "    model, tokenizer = load_model(model_name = model_name)\n",
    "    test_evaluation(model, tokenizer=tokenizer)\n",
    "    print(f\"Model : {model_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) Discussion\n",
    "\n",
    "Our evaluation results demonstrate the significant impact of advanced reasoning techniques like **Chain-of-Thought (CoT)**, **Latent Reasoning**, and **Chain-of-Thought Decoding** on the performance of the fine-tuned Gemma 2 model. The metrics reveal a clear progression in model accuracy and robustness across training stages, highlighting the effectiveness of these methods.\n",
    "\n",
    "#### Key Findings:\n",
    "1. **Accuracy Improvement**:\n",
    "   - **Stage 0**: Accuracy starts at **19%**, indicating the baseline performance before advanced reasoning techniques are fully applied.\n",
    "   - **Stage 1**: Accuracy jumps to **83%**, showcasing the immediate benefits of incorporating CoT and Latent Reasoning.\n",
    "   - **Stage 2**: Accuracy reaches **92%**, demonstrating the model's ability to refine its reasoning and decision-making processes further.\n",
    "\n",
    "2. **Fuzzy Score**:\n",
    "   - The fuzzy score improves from **20.35** in Stage 0 to **92.23** in Stage 2, indicating better semantic similarity and alignment with expected outputs.\n",
    "\n",
    "3. **BLEU Score**:\n",
    "   - The BLEU score increases from **0.17** in Stage 0 to **0.91** in Stage 2, reflecting significant improvements in the model's ability to generate linguistically accurate and coherent text.\n",
    "\n",
    "4. **BERTScore F1**:\n",
    "   - The BERTScore F1 improves from **0.62** in Stage 0 to **0.97** in Stage 2, confirming that the model's outputs are more contextually and semantically aligned with the ground truth.\n",
    "\n",
    "#### Metrics Summary:\n",
    "| **Stage**       | **Accuracy** | **Fuzzy Score** | **BLEU Score** | **BERTScore F1** |\n",
    "|------------------|--------------|-----------------|----------------|-------------------|\n",
    "| **Stage 0**      | 0.19         | 20.35           | 0.17           | 0.62              |\n",
    "| **Stage 1**      | 0.83         | 80.20           | 0.82           | 0.82              |\n",
    "| **Stage 2**      | 0.92         | 92.23           | 0.91           | 0.97              |\n",
    "\n",
    "#### Python Package:\n",
    "To make these advancements accessible, we've compiled the `LatentReasoningGemmaCausalLLM` class, along with the Chain-of-Thought Decoding implementation, into a Python package. You can install it via:\n",
    "```bash\n",
    "pip install git+https://github.com/vicksEmmanuel/latent-gemma.git\n",
    "```\n",
    "\n",
    "This package provides a user-friendly interface for leveraging the fine-tuned Gemma 2 model with advanced reasoning capabilities.\n",
    "\n",
    "\n",
    "Next steps, Let's upload the model to kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10) Upload the Model to Kaggle Models\n",
    "\n",
    "Step 1: \n",
    "- Go to the model folder <br/>\n",
    "- Find the config.json file <br/>\n",
    "- Replace the value of `_name_or_path` with the original kaggle path `google/gemma-2/transformers/gemma-2-2b` <br/>\n",
    "\n",
    "Step 2: \n",
    "- Turn the path to checkpoint to zip by running the command ` zip -r latent_gemma2_finetune.zip path-to-model/output_stage3/checkpoint-10000`\n",
    "\n",
    "\n",
    "Step 3: \n",
    "- Now, upload the .zip file to Kaggle Models.\n",
    "- Step 1: Go to Kaggle Models\n",
    "- Log in to your Kaggle account.\n",
    "- Navigate to the Kaggle Models page.\n",
    "\n",
    "Step 4: Create a New Model\n",
    "- Click on the \"New Model\" button.\n",
    "- Fill in the required details:\n",
    "<img src=\"https://res.cloudinary.com/vickie/image/upload/v1735816980/exvtxvjs7heemon26he2.png\" alt=\"Gemini Reasoning Finetuning\" width=\"1000\"/>\n",
    "\n",
    "- Click \"Upload.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10) Usage\n",
    "\n",
    "First Add the script to a setup.py file\n",
    "\n",
    "```\n",
    "import kagglehub\n",
    "\n",
    "kagglehub.login()\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.model_download(\"victorumesiobi/gemma-2-japanese-english-reasoning/transformers/1\")\n",
    "\n",
    "print(\"Path to model files:\", path)\n",
    "\n",
    "```\n",
    "\n",
    "Then Run\n",
    "\n",
    "`python setup.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q git+https://github.com/vicksEmmanuel/latent-gemma.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "from latent_gemma import LatentReasoningGemmaForCausalLM\n",
    "\n",
    "model_path = \"/home/featurize/.cache/kagglehub/models/victorumesiobi/gemma-2-japanese-english-reasoning/transformers/1/2\" # Replace with the path to which your model was downloaded too\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model_config = AutoConfig.from_pretrained(model_path)\n",
    "\n",
    "config = {\n",
    "    \"max_length\": 256\n",
    "}\n",
    "latent_config = LatentReasoningGemmaForCausalLM.DEFAULT_CONFIG\n",
    "LatentReasoningGemmaForCausalLM.DEFAULT_CONFIG = {\n",
    "    **latent_config,\n",
    "    **config\n",
    "}\n",
    "updated_latent_config = LatentReasoningGemmaForCausalLM.DEFAULT_CONFIG\n",
    "model = LatentReasoningGemmaForCausalLM(config=model_config)\n",
    "model = model.from_pretrained(model_path)\n",
    "model.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"人気漫画『ドラえもん』の登場人物で、ジャイアンの苗字は剛田ですが、スネ夫の苗字は何でしょう？\"\n",
    "output = model.generate_answer(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    question=text, \n",
    "    k=5, \n",
    "    max_length=256\n",
    ")\n",
    "\n",
    "print(f\"output: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you could directly use the Normal way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=32)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
